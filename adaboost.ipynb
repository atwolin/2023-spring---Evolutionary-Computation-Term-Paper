{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "import copy\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import gp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import KFold\n",
    "import deap.gp as gp\n",
    "from deap import creator, base, tools, algorithms\n",
    "from deap.gp import cxOnePoint as cx_simple\n",
    "from deap.gp import PrimitiveSet\n",
    "from data import get_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD2VEC = \"word2vec\"\n",
    "GLOVE = \"glove\"\n",
    "FASTTEXT = \"fasttext\"\n",
    "CX_RANDOM = 0\n",
    "CX_SIMPLE = 1\n",
    "CX_UNIFORM = 2\n",
    "CX_FAIR = 3\n",
    "CX_ONEPOINT = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic operators\n",
    "def protected_div(x, y):\n",
    "    mask = y == 0\n",
    "    safe_y = np.where(mask, 1, y)\n",
    "    return np.where(mask, 1, x / safe_y)\n",
    "\n",
    "\n",
    "def protected_sqrt(x):\n",
    "    sign = np.sign(x)\n",
    "    x = np.abs(x)\n",
    "    return np.sqrt(x) * sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossover method\n",
    "def subtree_height(tree, index):\n",
    "        \"\"\"\n",
    "        Calculate the height of the subtree starting at the given index.\n",
    "        \"\"\"\n",
    "        def height(node_index):\n",
    "            node = tree[node_index]\n",
    "            if node.arity == 0:  # Leaf node\n",
    "                return 1\n",
    "            else:\n",
    "                return 1 + max(\n",
    "                    height(child_index)\n",
    "                    for child_index in range(\n",
    "                        node_index + 1, node_index + 1 + node.arity\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return height(index)\n",
    "\n",
    "def searchSubtree_idx(tree, begin):\n",
    "        end = begin + 1\n",
    "        total = tree[begin].arity\n",
    "        while total > 0:\n",
    "            total += tree[end].arity - 1\n",
    "            end += 1\n",
    "        return begin, end\n",
    "\n",
    "def cx_uniform(ind1, ind2):\n",
    "        # No crossover on single node tree\n",
    "        if (len(ind1) < 2 or len(ind2) < 2):\n",
    "            return ind1, ind2\n",
    "\n",
    "        child = type(ind1)([])\n",
    "        parents = [ind1, ind2]\n",
    "        flag0, flag1 = 0, 0\n",
    "        left_0 = parents[0].searchSubtree(1)\n",
    "        left_1 = parents[1].searchSubtree(1)\n",
    "        b0, e0 = searchSubtree_idx(parents[0], 1)\n",
    "        b1, e1 = searchSubtree_idx(parents[1], 1)\n",
    "\n",
    "        if e0 + 1 < len(parents[0]):\n",
    "            right_0 = parents[0].searchSubtree(e0 + 1)\n",
    "            flag0 = 1\n",
    "        if e1 + 1 < len(parents[1]):\n",
    "            right_1 = parents[1].searchSubtree(e1 + 1)\n",
    "            flag1 = 1\n",
    "        left = [left_0, left_1]\n",
    "        if flag0 == 1 and flag1 == 1:\n",
    "            right = [right_0, right_1]\n",
    "            r_arity = 0\n",
    "            if parents[0][e0 + 1].arity == parents[1][e1 + 1].arity:\n",
    "                r_arity = 1\n",
    "        r = random.randint(0, 1)  # root\n",
    "        m = 1 - r\n",
    "        if len(parents[r]) < len(parents[m]):\n",
    "            # root = parents[r].root\n",
    "            if flag1 == 0 or flag0 == 0:\n",
    "                return parents[r], parents[m]\n",
    "            parents[m][0] = parents[r].root\n",
    "            m = r\n",
    "        if flag0 == 1 and flag1 == 1:\n",
    "            r1 = random.randint(0, 1)  # 左邊\n",
    "            if parents[r][1] == parents[r1][1]:\n",
    "                parents[r][left[r]] = parents[r1][left[r1]]\n",
    "            if r_arity == 1:\n",
    "                r2 = random.randint(0, 1)\n",
    "                parents[r][right[r]] = parents[r2][right[r2]]\n",
    "        else:\n",
    "            # print(\"只有一個子點\")\n",
    "            r1 = random.randint(0, 1)\n",
    "            parents[r][left[r1]] = parents[r1][left[r1]]\n",
    "        return parents[r], parents[r]\n",
    "\n",
    "def cx_fair(ind1, ind2):\n",
    "    \"\"\"Size fair crossover for two trees.\n",
    "    :param ind1: First tree participating in the crossover.\n",
    "    :param ind2: Second tree participating in the crossover.\n",
    "    :returns: A tuple of two trees.\n",
    "    \"\"\"\n",
    "    # No crossover on single node tree\n",
    "    if len(ind1) < 2 or len(ind2) < 2:\n",
    "        return ind1, ind2\n",
    "\n",
    "    # List all available primitive types in each individual\n",
    "    types1 = gp.defaultdict(list)\n",
    "    types2 = gp.defaultdict(list)\n",
    "    if ind1.root.ret == gp.__type__:\n",
    "        # Not STGP optimization\n",
    "        types1[gp.__type__] = list(range(1, len(ind1)))\n",
    "        types2[gp.__type__] = list(range(1, len(ind2)))\n",
    "        common_types = [gp.__type__]\n",
    "    else:\n",
    "        for idx, node in enumerate(ind1[1:], 1):\n",
    "            types1[node.ret].append(idx)\n",
    "        for idx, node in enumerate(ind2[1:], 1):\n",
    "            types2[node.ret].append(idx)\n",
    "        common_types = set(types1.keys()).intersection(set(types2.keys()))\n",
    "\n",
    "    if len(common_types) > 0:\n",
    "        type_ = random.choice(list(common_types))\n",
    "\n",
    "    index1 = random.choice(types1[type_])\n",
    "    height1 = subtree_height(ind1, index1)\n",
    "    # height = ind1.height\n",
    "\n",
    "    while True:\n",
    "        index2 = random.choice(types2[type_])\n",
    "        height2 = subtree_height(ind2, index2)\n",
    "        if height2 <= height1:\n",
    "            # print(f\"height1: {height1}, height2: {height2}\")\n",
    "            break\n",
    "    slice1 = ind1.searchSubtree(index1)\n",
    "    slice2 = ind2.searchSubtree(index2)\n",
    "    ind1[slice1], ind2[slice2] = ind2[slice2], ind1[slice1]\n",
    "    return ind1, ind2\n",
    "\n",
    "def traverse_tree(stack, res, parent, idx):\n",
    "    while res != 0:\n",
    "        res -= 1\n",
    "        idx += 1\n",
    "        stack.append((parent[idx], [], idx))\n",
    "        res += parent[idx].arity\n",
    "    # print(f\"stack: {stack}\")\n",
    "    return stack, res, idx\n",
    "\n",
    "def cx_one_point(ind1, ind2):\n",
    "    idx1 = 0\n",
    "    idx2 = 0\n",
    "    # To track the trees\n",
    "    stack1 = []\n",
    "    stack2 = []\n",
    "    # Store the common region\n",
    "    region1 = []\n",
    "    region2 = []\n",
    "\n",
    "    # Start traversing the trees\n",
    "    while idx1 < len(ind1) and idx2 < len(ind2):\n",
    "        # Push the nodes to the stack\n",
    "        stack1.append((ind1[idx1], [], idx1))\n",
    "        stack2.append((ind2[idx2], [], idx2))\n",
    "\n",
    "        # Not the same region\n",
    "        if stack1[-1][0].arity != stack2[-1][0].arity:\n",
    "            res1 = stack1[-1][0].arity\n",
    "            res2 = stack2[-1][0].arity\n",
    "            stack1, res1, idx1 = traverse_tree(stack1, res1, ind1, idx1)\n",
    "            stack2, res2, idx2 = traverse_tree(stack2, res2, ind2, idx2)\n",
    "        else:\n",
    "            region1.append([ind1[idx1], idx1])\n",
    "            region2.append([ind2[idx2], idx2])\n",
    "\n",
    "        idx1 += 1\n",
    "        idx2 += 1\n",
    "\n",
    "    # for pri, idx in region1:\n",
    "    #     print(f\"{idx}: {pri.name}\")\n",
    "\n",
    "    # Select crossover point\n",
    "    if len(region1) > 0:\n",
    "        point = random.randint(0, len(region1) - 1)\n",
    "        # print(f\"crossover point: {point}\")\n",
    "        # print(f\"crossover point for trees: {region1[point]}, {region2[point]}\")\n",
    "\n",
    "    # Swap subtrees\n",
    "    if len(region1) > 0:\n",
    "        slice1 = ind1.searchSubtree(region1[point][1])\n",
    "        slice2 = ind2.searchSubtree(region2[point][1])\n",
    "        ind1[slice1], ind2[slice2] = ind2[slice2], ind1[slice1]\n",
    "\n",
    "    return ind1, ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP():\n",
    "    def __init__(self, embeddings, dimension, population_size, crossover_method, cross_prob, mut_prob, num_generations, dataset):\n",
    "        self.embeddings = embeddings\n",
    "        self.dim = dimension\n",
    "        self.pop_size = population_size\n",
    "        self.pop = None\n",
    "        self.cx_method = crossover_method\n",
    "        self.cx_pb = cross_prob\n",
    "        self.mut_pb = mut_prob\n",
    "        self.num_gen = 0\n",
    "        self.inputword = dataset[0].str.split().apply(lambda x: x[:5])\n",
    "        self.realword = dataset[0].str.split().str.get(5)\n",
    "        self.eval_count = 0\n",
    "\n",
    "    def register(self):\n",
    "        # Function set\n",
    "        self.pset = gp.PrimitiveSet(\"MAIN\", 5)\n",
    "        self.pset.addPrimitive(np.add, 2)\n",
    "        self.pset.addPrimitive(np.subtract, 2)\n",
    "        self.pset.addPrimitive(np.multiply, 2)\n",
    "        self.pset.addPrimitive(protected_div, 2)\n",
    "        self.pset.addPrimitive(protected_sqrt, 1)\n",
    "        self.pset.addPrimitive(np.square, 1)\n",
    "        # Terminal set\n",
    "        count = 0\n",
    "        self.pset.renameArguments(ARG0=\"a\", ARG1=\"b\", ARG2=\"c\", ARG3=\"d\", ARG4=\"e\")\n",
    "        # for line in self.inputword:\n",
    "            # for word in line:\n",
    "                # if count < 5:\n",
    "                    # print(f\"conut: {count}\")\n",
    "                    # print(f\"embedding: {embeddings[word]}, {type(embeddings[word])}\")\n",
    "                    # count += 1\n",
    "                # self.pset.addTerminal(word)\n",
    "\n",
    "        # Initialize the individual\n",
    "        if \"Individual\" not in creator.__dict__:\n",
    "            creator.create(\"FitnessMax\", base.Fitness, weights=(1,))\n",
    "            creator.create(\n",
    "                \"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMax, pset=self.pset\n",
    "            )\n",
    "\n",
    "        # Initialize the toolbox\n",
    "        self.toolbox = base.Toolbox()\n",
    "        self.toolbox.register(\"expr\", gp.genHalfAndHalf, pset=self.pset, min_=1, max_=5)\n",
    "        self.toolbox.register(\n",
    "            \"individual\", tools.initIterate, creator.Individual, self.toolbox.expr\n",
    "        )\n",
    "        self.toolbox.register(\n",
    "            \"population\",\n",
    "            tools.initRepeat,\n",
    "            list,\n",
    "            self.toolbox.individual,\n",
    "            n=self.pop_size,\n",
    "        )\n",
    "\n",
    "        # Register the operators\n",
    "        # self.toolbox.register(\"select_candidate\", tools.selRandom, tournsize=3)\n",
    "        self.toolbox.register(\"crossover\", self.crossover)\n",
    "        self.toolbox.register(\n",
    "            \"mutate\", gp.mutUniform, expr=self.toolbox.expr, pset=self.pset\n",
    "        )\n",
    "        self.toolbox.decorate(\n",
    "            \"mutate\", gp.staticLimit(operator.attrgetter(\"height\"), max_value=5)\n",
    "        )\n",
    "        self.toolbox.register(\"evaluate\", self.evaluate)\n",
    "\n",
    "        # Register the record for analyzing\n",
    "        self.stats = tools.Statistics(key=lambda ind: ind.fitness.values)\n",
    "        self.stats.register(\"avg\", np.mean)\n",
    "        self.stats.register(\"std\", np.std)\n",
    "        self.stats.register(\"min\", np.min)\n",
    "        self.stats.register(\"max\", np.max)\n",
    "        self.hof = tools.HallOfFame(10)  # hall of fame size\n",
    "\n",
    "    def clean_data(self, data):\n",
    "        data = np.where(np.isinf(data), np.finfo(np.float32).max, data)\n",
    "        data = np.nan_to_num(data, nan=0.0)\n",
    "        return data\n",
    "\n",
    "    def evaluate(self, individual, input_word):\n",
    "        \"\"\"Evalute the fitness of an individual\"\"\"\n",
    "        # print(f\"individual種類:{type(individual)}\")\n",
    "        func = gp.compile(individual, self.pset)\n",
    "        total_similarity = 0.0\n",
    "        for data_index in range(len(input_word)):\n",
    "            words = self.inputword.iloc[data_index]\n",
    "            in_vectors = [self.embeddings[word] for word in words]\n",
    "            a, b, c, d, e = in_vectors[:5]\n",
    "\n",
    "            y = self.realword.iloc[data_index]\n",
    "            out_vector = self.embeddings[y]\n",
    "\n",
    "            predict = self.clean_data(func(a, b, c, d, e))\n",
    "            similarity = cosine_similarity([predict], [out_vector])[0][0]\n",
    "            total_similarity += similarity\n",
    "\n",
    "        fitness = total_similarity / len(self.inputword)\n",
    "        ftiness = self.clean_data(fitness)\n",
    "        self.eval_count += 1\n",
    "        return (fitness,)\n",
    "\n",
    "    def crossover(self, ind1, ind2):\n",
    "        parents = [ind1, ind2]\n",
    "        if random.uniform(0, 1) < self.cx_pb:\n",
    "            if self.cx_method == CX_RANDOM:\n",
    "                choice = random.randint(1, 4)\n",
    "            if choice == CX_SIMPLE or self.cx_method == CX_SIMPLE:\n",
    "                ind1, ind2 = cx_simple(ind1, ind2)\n",
    "            elif choice == CX_UNIFORM or self.cx_method == CX_UNIFORM:\n",
    "                ind1, ind2 = cx_uniform(ind1, ind2)\n",
    "            elif choice == CX_ONEPOINT or self.cx_method == CX_FAIR:\n",
    "                ind1, ind2 = cx_fair(ind1, ind2)\n",
    "            else:  # self.cx_method == 4:\n",
    "                ind1, ind2 = cx_one_point(ind1, ind2)\n",
    "        # return ind1, ind2\n",
    "\n",
    "        fitness_ind1 = self.toolbox.evaluate(ind1, self.realword)\n",
    "        # ?:\n",
    "        # if self.cx_method == 2:\n",
    "        #     parents.remove(b)\n",
    "        #     return parents\n",
    "        fitness_ind2 = self.toolbox.evaluate(ind2, self.realword)\n",
    "        if fitness_ind1 <= fitness_ind2:\n",
    "            parents.remove(ind1)\n",
    "            return parents\n",
    "        else:\n",
    "            parents.remove(ind2)\n",
    "            return parents\n",
    "\n",
    "    def mutate(self, offspring):\n",
    "        if random.uniform(0, 1) < self.mut_pb:\n",
    "            self.toolbox.mutate(offspring[0])\n",
    "            offspring[0].fitness.values = self.toolbox.evaluate(offspring[0], self.realword)\n",
    "        return offspring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Boosting parameters\n",
    "boosting_interval = 20\n",
    "ensemble = []\n",
    "\n",
    "population_size = 250\n",
    "dim = 10\n",
    "crossover_method = CX_RANDOM\n",
    "cross_prob = 1\n",
    "mut_prob = 0.1\n",
    "num_generations = 100\n",
    "embedding = WORD2VEC\n",
    "\n",
    "dataset, embeddings, word2vec_model = get_embeddings(WORD2VEC, 10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258316</th>\n",
       "      <td>federal election fact check underemployment sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210027</th>\n",
       "      <td>remote health services get staffing boost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144075</th>\n",
       "      <td>grandstand at stumps gabba day five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168439</th>\n",
       "      <td>sorenstam lands fifth major after playoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120555</th>\n",
       "      <td>bihar flood exposes vulnerable children to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "258316  federal election fact check underemployment sh...\n",
       "210027          remote health services get staffing boost\n",
       "144075                grandstand at stumps gabba day five\n",
       "168439          sorenstam lands fifth major after playoff\n",
       "120555         bihar flood exposes vulnerable children to"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the population\n",
    "trees = GP(embeddings, dim, population_size, crossover_method, cross_prob, mut_prob, num_generations, dataset)\n",
    "trees.register()\n",
    "\n",
    "trees.pop = trees.toolbox.population(n = trees.pop_size)\n",
    "# Evaluate the entire population\n",
    "fitnesses = map(trees.toolbox.evaluate, trees.pop, trees.inputword)\n",
    "for ind, fit in zip(trees.pop, fitnesses):\n",
    "    ind.fitness.values = fit\n",
    "\n",
    "# Initialize instance weights\n",
    "dataset[\"weights\"] = 1.0 / len(dataset)\n",
    "dataset[\"weights_update\"] = 1.0 / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting at iteration 0\n",
      "{'avg': 0.0008414324622615817, 'std': 0.010877532362023977, 'min': -0.0008253868586921478, 'max': 0.17230871923071736}\n",
      "new_weight: 0.20817993765811826\n",
      "Boosting at iteration 20\n",
      "{'avg': 0.002688088464772753, 'std': 0.021076442611122865, 'min': -0.0008253868586921478, 'max': 0.2784130670258903}\n",
      "new_weight: 0.3858344078909196\n",
      "Boosting at iteration 40\n",
      "{'avg': 0.008985940092153791, 'std': 0.053407719922331134, 'min': -0.0008253868586921478, 'max': 0.42328820737812783}\n",
      "new_weight: 0.7339683578408494\n",
      "Evaloation 1150\n",
      "{'avg': 0.003943132394234313, 'std': 0.033369355727966746, 'min': -0.0008253868586921478, 'max': 0.40309313294350746}\n",
      "Boosting at iteration 60\n",
      "{'avg': 0.007751041929401153, 'std': 0.046627464304589174, 'min': -0.0008253868586921478, 'max': 0.40309313294350746}\n",
      "new_weight: 0.6753032259978303\n",
      "Boosting at iteration 80\n",
      "{'avg': 0.0048788658784267155, 'std': 0.02905386484506769, 'min': -0.0008253868586921478, 'max': 0.3066233919588794}\n",
      "new_weight: 0.44221767565123166\n",
      "Evaloation 1800\n",
      "{'avg': 0.006563600712509025, 'std': 0.0400533159471022, 'min': -0.0008253868586921478, 'max': 0.3856747871624093}\n"
     ]
    }
   ],
   "source": [
    "ensemble = []\n",
    "# Start the evolution\n",
    "while trees.num_gen < num_generations:\n",
    "    # Parent selection\n",
    "    candidates = tools.selRandom(trees.pop, 3)\n",
    "    sorted_candidates = sorted(candidates, key=lambda x: x.fitness.values)  # Small to large\n",
    "    # print(f\"sorted_candidates: {[ind.fitness.values for ind in sorted_candidates]}\")\n",
    "\n",
    "    # Crossover\n",
    "    parent1, parent2 = copy.deepcopy(candidates[0]), copy.deepcopy(candidates[1])\n",
    "    offspring = trees.toolbox.crossover(parent1, parent2)\n",
    "\n",
    "    # Mutation\n",
    "    offspring = trees.toolbox.mutate(offspring[0])\n",
    "\n",
    "    # Survival selection\n",
    "    offspring[0].fitness.values = trees.toolbox.evaluate(offspring[0], trees.realword)\n",
    "    if offspring[0].fitness.values > sorted_candidates[2].fitness.values:\n",
    "        idx = trees.pop.index(sorted_candidates[2])\n",
    "        trees.pop[idx] = offspring[0]\n",
    "\n",
    "    if trees.num_gen % boosting_interval == 0:\n",
    "        print(f\"Boosting at iteration {trees.num_gen}\")\n",
    "        record = trees.stats.compile(trees.pop)\n",
    "        trees.hof.update(trees.pop)\n",
    "        print(record)\n",
    "        # total_weights = np.sum(dataset[\"weights\"])\n",
    "        errors = []\n",
    "        for data_index in range(len(dataset)):\n",
    "            func = gp.compile(ind, trees.pset)\n",
    "            a, b, c, d, e = [trees.embeddings[word] for word in trees.inputword.iloc[1]]\n",
    "            y_pred = (func(a, b, c, d, e))\n",
    "            y_true = (trees.embeddings[trees.realword.iloc[data_index]])\n",
    "\n",
    "            # Calculate the prediction error (e.g., Euclidean distance)\n",
    "            error = np.linalg.norm(y_true - y_pred)\n",
    "            # print(f\"error: {error}\")\n",
    "            errors.append(error)\n",
    "        # print(f\"errors: {len(errors)}\")\n",
    "\n",
    "        # Normalize errors\n",
    "        max_error = max(errors)\n",
    "        if max_error == 0:\n",
    "            max_error = 1\n",
    "        errors = [error / max_error for error in errors]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Get best individual\n",
    "        best = max(trees.pop, key=lambda x: x.fitness.values)\n",
    "        epsilon = 1 - best.fitness.values[0]\n",
    "        best_weight = np.log((1 - epsilon) / epsilon)\n",
    "        ensemble.append(best)\n",
    "        new_weight = np.exp(best_weight)\n",
    "        print(f\"new_weight: {new_weight}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Update instance weights\n",
    "        dataset[\"weights_update\"] *= np.exp(errors) * new_weight\n",
    "        # Normalize weights\n",
    "        sum_weights = np.sum(dataset[\"weights_update\"])\n",
    "        dataset[\"weights_update\"] = dataset[\"weights_update\"] / sum_weights\n",
    "\n",
    "        # Update the population dataset\n",
    "        select_new_data = np.random.uniform(0, 1, len(dataset))\n",
    "        dataset[\"cumulative_weights\"] = dataset[\"weights_update\"].cumsum()\n",
    "        # Find the indices of the closest rows in cumulative_weights for each value in select_new_data\n",
    "        indices = np.digitize(select_new_data, dataset[\"cumulative_weights\"])\n",
    "        # Create new dataset by selecting rows from original dataset based on indices\n",
    "        new_dataset = dataset.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "        trees.dataset = new_dataset\n",
    "        # Evaluate the entire population\n",
    "        fitnesses = map(trees.toolbox.evaluate, trees.pop, trees.inputword)\n",
    "        for ind, fit in zip(trees.pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "    # Show the record\n",
    "    if trees.eval_count % 50 == 0:\n",
    "        print(f\"Evaloation {trees.eval_count}\")\n",
    "        record = trees.stats.compile(trees.pop)\n",
    "        trees.hof.update(trees.pop)\n",
    "        print(record)\n",
    "    trees.num_gen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg': 0.008515772810846471, 'std': 0.04964755771014911, 'min': -0.0008674927723010232, 'max': 0.40897962526391574}\n"
     ]
    }
   ],
   "source": [
    "record = trees.stats.compile(trees.pop)\n",
    "trees.hof.update(trees.pop)\n",
    "print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>weights</th>\n",
       "      <th>weights_update</th>\n",
       "      <th>cumulative_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142305</th>\n",
       "      <td>contracts being finalised for gold mine</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133777</th>\n",
       "      <td>sa naplan results behind national average</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10775</th>\n",
       "      <td>captain detained over bahrain boat accident</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214874</th>\n",
       "      <td>gladstone ports corporation to boost jobs</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.001499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139083</th>\n",
       "      <td>messi wins world cup golden ball</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.001873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0   weights  weights_update  \\\n",
       "142305      contracts being finalised for gold mine  0.000375        0.000375   \n",
       "133777    sa naplan results behind national average  0.000375        0.000375   \n",
       "10775   captain detained over bahrain boat accident  0.000375        0.000375   \n",
       "214874    gladstone ports corporation to boost jobs  0.000375        0.000375   \n",
       "139083             messi wins world cup golden ball  0.000375        0.000375   \n",
       "\n",
       "        cumulative_weights  \n",
       "142305            0.000375  \n",
       "133777            0.000749  \n",
       "10775             0.001124  \n",
       "214874            0.001499  \n",
       "139083            0.001873  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_new_data = np.random.uniform(0, 1, len(dataset))\n",
    "dataset[\"cumulative_weights\"] = dataset[\"weights_update\"].cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure cumulative_weights is sorted\n",
    "# dataset.sort_values(\"cumulative_weights\", inplace=True)\n",
    "\n",
    "# Find the indices of the closest rows in cumulative_weights for each value in select_new_data\n",
    "indices = np.digitize(select_new_data, dataset[\"cumulative_weights\"])\n",
    "\n",
    "# Create new dataset by selecting rows from original dataset based on indices\n",
    "new_dataset = dataset.iloc[indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>weights</th>\n",
       "      <th>weights_update</th>\n",
       "      <th>cumulative_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nsw premier accepts mike gallachers resignation</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.100037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advisor resigns over conflict of interest</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.917197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>broke residents call for blast action</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.440614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funeral service held for philippines landslide</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.306107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationals call for bushfire evacuation database</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.368303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>crews to begin south coast backburning</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.858749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>the fight for the kitchen table</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.480704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>police investigate ute rampage northern victoria</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.619333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>man pleads guilty over child porn</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.217310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>bakhtiyari famliy packs bags for deportation</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.395279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2669 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0   weights  \\\n",
       "0      nsw premier accepts mike gallachers resignation  0.000375   \n",
       "1            advisor resigns over conflict of interest  0.000375   \n",
       "2                broke residents call for blast action  0.000375   \n",
       "3       funeral service held for philippines landslide  0.000375   \n",
       "4      nationals call for bushfire evacuation database  0.000375   \n",
       "...                                                ...       ...   \n",
       "2664            crews to begin south coast backburning  0.000375   \n",
       "2665                   the fight for the kitchen table  0.000375   \n",
       "2666  police investigate ute rampage northern victoria  0.000375   \n",
       "2667                 man pleads guilty over child porn  0.000375   \n",
       "2668      bakhtiyari famliy packs bags for deportation  0.000375   \n",
       "\n",
       "      weights_update  cumulative_weights  \n",
       "0           0.000375            0.100037  \n",
       "1           0.000375            0.917197  \n",
       "2           0.000375            0.440614  \n",
       "3           0.000375            0.306107  \n",
       "4           0.000375            0.368303  \n",
       "...              ...                 ...  \n",
       "2664        0.000375            0.858749  \n",
       "2665        0.000375            0.480704  \n",
       "2666        0.000375            0.619333  \n",
       "2667        0.000375            0.217310  \n",
       "2668        0.000375            0.395279  \n",
       "\n",
       "[2669 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "973"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_weight: 0.0008214122571255709\n"
     ]
    }
   ],
   "source": [
    "# Get best individual\n",
    "best = max(trees.pop, key=lambda x: x.fitness.values)\n",
    "epsilon = 1 - best.fitness.values[0]\n",
    "best_weight = np.log((1 - epsilon) / epsilon)\n",
    "ensemble.append(best)\n",
    "new_weight = np.exp(best_weight)\n",
    "print(f\"new_weight: {new_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23729</th>\n",
       "      <td>council concerned about paringa riverbank erosion</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76530</th>\n",
       "      <td>controversial russian artist disappears in berlin</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257892</th>\n",
       "      <td>simplot increases prices for pea growers</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93799</th>\n",
       "      <td>pacific national train drivers cancel planned</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237328</th>\n",
       "      <td>thousands of south australians stranded waiting</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0   weights\n",
       "23729   council concerned about paringa riverbank erosion  0.000375\n",
       "76530   controversial russian artist disappears in berlin  0.000375\n",
       "257892           simplot increases prices for pea growers  0.000375\n",
       "93799       pacific national train drivers cancel planned  0.000375\n",
       "237328    thousands of south australians stranded waiting  0.000375"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the outputs of the GP trees\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for ind in trees.pop:\n",
    "    func = gp.compile(ind, trees.pset)\n",
    "    a, b, c, d, e = [trees.embeddings[word] for word in trees.inputword.iloc[1]]\n",
    "    y_pred.append(func(a, b, c, d, e))\n",
    "    y_true.append(trees.embeddings[trees.realword.iloc[data_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the best individual from the current population\n",
    "best_individual = tools.selBest(trees.pop, 1)[0]\n",
    "func = gp.compile(best_individual, trees.pset)\n",
    "a, b, c, d, e = [trees.embeddings[word] for word in trees.inputword.iloc[1]]\n",
    "predict = trees.clean_data(func(a, b, c, d, e))\n",
    "predict = predict / np.linalg.norm(predict) if np.linalg.norm(predict) != 0 else predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34.80443, 35.789825, 37.67824, 36.81569, 38.063366]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def update_instance_weights(trees, dataset):\n",
    "total_weights = np.sum(dataset[\"weights\"])\n",
    "errors = []\n",
    "\n",
    "for data_index in range(len(dataset)):\n",
    "    func = gp.compile(ind, trees.pset)\n",
    "    a, b, c, d, e = [trees.embeddings[word] for word in trees.inputword.iloc[1]]\n",
    "    y_pred = (func(a, b, c, d, e))\n",
    "    y_true = (trees.embeddings[trees.realword.iloc[data_index]])\n",
    "\n",
    "    # Calculate the prediction error (e.g., Euclidean distance)\n",
    "    error = np.linalg.norm(y_true - y_pred)\n",
    "    # print(f\"error: {error}\")\n",
    "    errors.append(error)\n",
    "errors[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.79609025, 0.81862944, 0.8618237, 0.8420943, 0.8706327]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize errors\n",
    "max_error = max(errors)\n",
    "if max_error == 0:\n",
    "    max_error = 1\n",
    "errors = [error / max_error for error in errors]\n",
    "errors[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2168567, 2.26739  , 2.3674743, ..., 2.334882 , 2.2394364,\n",
       "       2.3770418], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights\n",
    "for i in range(len(dataset)):\n",
    "    dataset['weights_update'] = dataset[\"weights\"] * np.exp(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>weights</th>\n",
       "      <th>weights_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23729</th>\n",
       "      <td>council concerned about paringa riverbank erosion</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76530</th>\n",
       "      <td>controversial russian artist disappears in berlin</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257892</th>\n",
       "      <td>simplot increases prices for pea growers</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93799</th>\n",
       "      <td>pacific national train drivers cancel planned</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237328</th>\n",
       "      <td>thousands of south australians stranded waiting</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0   weights  \\\n",
       "23729   council concerned about paringa riverbank erosion  0.000375   \n",
       "76530   controversial russian artist disappears in berlin  0.000375   \n",
       "257892           simplot increases prices for pea growers  0.000375   \n",
       "93799       pacific national train drivers cancel planned  0.000375   \n",
       "237328    thousands of south australians stranded waiting  0.000375   \n",
       "\n",
       "        weights_update  \n",
       "23729         0.000831  \n",
       "76530         0.000850  \n",
       "257892        0.000887  \n",
       "93799         0.000870  \n",
       "237328        0.000895  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.344232827726488"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize weights\n",
    "sum_weights = np.sum(dataset[\"weights_update\"])\n",
    "sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"weights_update\"] /= sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>weights</th>\n",
       "      <th>weights_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23729</th>\n",
       "      <td>council concerned about paringa riverbank erosion</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76530</th>\n",
       "      <td>controversial russian artist disappears in berlin</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257892</th>\n",
       "      <td>simplot increases prices for pea growers</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93799</th>\n",
       "      <td>pacific national train drivers cancel planned</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237328</th>\n",
       "      <td>thousands of south australians stranded waiting</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0   weights  \\\n",
       "23729   council concerned about paringa riverbank erosion  0.000375   \n",
       "76530   controversial russian artist disappears in berlin  0.000375   \n",
       "257892           simplot increases prices for pea growers  0.000375   \n",
       "93799       pacific national train drivers cancel planned  0.000375   \n",
       "237328    thousands of south australians stranded waiting  0.000375   \n",
       "\n",
       "        weights_update  \n",
       "23729         0.000354  \n",
       "76530         0.000362  \n",
       "257892        0.000378  \n",
       "93799         0.000371  \n",
       "237328        0.000382  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
