{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import gp as deap_gp\n",
    "import gp\n",
    "from data import get_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1126\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        algorithm,\n",
    "        embedding_type,\n",
    "        dimension,\n",
    "        population_size,\n",
    "        crossover_method,\n",
    "        cross_prob,\n",
    "        mut_prob,\n",
    "        num_generations,\n",
    "        num_evaluations,\n",
    "        debug,\n",
    "    ):\n",
    "        self.algorithm = algorithm\n",
    "        self.embedding_type = embedding_type\n",
    "        self.dimension = dimension\n",
    "        self.population_size = population_size\n",
    "        self.crossover_method = crossover_method\n",
    "        self.cross_prob = cross_prob\n",
    "        self.mut_prob = mut_prob\n",
    "        self.num_generations = num_generations\n",
    "        self.num_evaluations = num_evaluations\n",
    "        self.debug = debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = config(\"simple_gp\",\n",
    "        \"word2vec\",\n",
    "        10,\n",
    "        100,\n",
    "        \"cx_random\",\n",
    "        0.9,\n",
    "        0.1,\n",
    "        100,\n",
    "        1000,\n",
    "        False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "data, embeddings, embedding_model = get_embeddings(\n",
    "        Config.embedding_type, Config.dimension\n",
    "    )\n",
    "\n",
    "cx_method = gp.get_cx_num(Config.crossover_method)\n",
    "\n",
    "# Initialize instance weights\n",
    "data[\"weights\"] = 1.0 / len(data)\n",
    "data[\"weights_update\"] = 1.0 / len(data)\n",
    "\n",
    "boosting_interval = 10  # Boosting interval\n",
    "\n",
    "ensemble = []  # Ensemble to store the best individuals\n",
    "\n",
    "gpab = gp.GP(\n",
    "    Config.algorithm,\n",
    "    Config.embedding_type,\n",
    "    Config.dimension,\n",
    "    Config.population_size,\n",
    "    cx_method,\n",
    "    Config.cross_prob,\n",
    "    Config.mut_prob,\n",
    "    Config.num_generations,\n",
    "    Config.num_evaluations,\n",
    "    data,\n",
    "    embeddings,\n",
    ")\n",
    "gpab.initialize_pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(trees):\n",
    "        X_list = np.array([np.array([trees.embeddings[char] for char in words]) for words in trees.inputword])\n",
    "        return X_list\n",
    "\n",
    "def get_y(trees):\n",
    "    y_true_list = np.array(np.array([trees.embeddings[char] for char in trees.realword]))\n",
    "    return y_true_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import copy\n",
    "from deap import tools\n",
    "\n",
    "class EvolvingTreeRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, trees):\n",
    "        self.trees = trees\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Assuming `trees` is an object that contains your population (pop),\n",
    "        # toolbox, and other necessary components for evolution.\n",
    "\n",
    "        # Your evolution logic here\n",
    "        candidates = tools.selRandom(self.trees.pop, 3)\n",
    "        sorted_candidates = sorted(candidates, key=lambda x: x.fitness.values)  # Small to large\n",
    "\n",
    "        parent1, parent2 = copy.deepcopy(candidates[0]), copy.deepcopy(candidates[1])\n",
    "        offspring = self.trees.toolbox.crossover(parent1, parent2)\n",
    "\n",
    "        offspring = self.trees.toolbox.mutate(offspring[0])\n",
    "\n",
    "        offspring[0].fitness.values = self.trees.toolbox.evaluate(offspring[0], self.trees.realword)\n",
    "        if offspring[0].fitness.values > sorted_candidates[2].fitness.values:\n",
    "            idx = self.trees.pop.index(sorted_candidates[2])\n",
    "            self.trees.pop[idx] = offspring[0]\n",
    "\n",
    "        # Note: You need to adapt this method to work with X and y if they are to be used.\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Implement prediction logic based on evolved trees\n",
    "        # This is a placeholder; you need to adapt it to your specific case.\n",
    "        return np.zeros(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m regr \u001b[38;5;241m=\u001b[39m GPABRegressor(evolving_tree_regressor, n_estimators\u001b[38;5;241m=\u001b[39mgpab\u001b[38;5;241m.\u001b[39mpop_size)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fit AdaBoostRegressor\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mregr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[135], line 56\u001b[0m, in \u001b[0;36mGPABRegressor.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    Build a boosted ensemble of estimators from the training set (X, y).\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     sample_weight \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintial sample weight: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_weight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Clear any previous fit\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "# Assuming `trees` is your evolutionary algorithm setup\n",
    "evolving_tree_regressor = EvolvingTreeRegressor(gpab)\n",
    "\n",
    "# Create AdaBoostRegressor with the custom base estimator\n",
    "regr = GPABRegressor(evolving_tree_regressor, n_estimators=gpab.pop_size)\n",
    "\n",
    "# Fit AdaBoostRegressor\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPABRegressor():\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: deap.gp object\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "\n",
    "    n_estimators: int\n",
    "        The number of estimators to train, a.k.a. the number of population of GP trees.\n",
    "\n",
    "    learning_rate: float, default=1.0\n",
    "        The learning rate of the boosting algorithm.\n",
    "\n",
    "    loss: {'linear', 'square', 'exponential'}, optional\n",
    "        The loss function to use when updating the weights after each iteration.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimators_: estimator\n",
    "        The base estimator from which the ensemble is grown.\n",
    "\n",
    "    estimators_: list of regressors\n",
    "        The collection of fitted sub-estimators.\n",
    "\n",
    "    estimator_weights_: array-like of shape (n_estimators,)\n",
    "        Weights for each estimator in the boosted ensemble.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, n_estimators, learning_rate=1, loss=\"linear\"):\n",
    "        self.estimator = estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Build a boosted ensemble of estimators from the training set (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array\n",
    "            The training input samples (length = 5)\n",
    "\n",
    "        y: array\n",
    "            The target values (real numbers)\n",
    "\n",
    "        sample_weight: array\n",
    "            The sample weights. If None, the sample weights are initialized to 1 / n_samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        sample_weight /= sample_weight.sum()\n",
    "        print(f\"intial sample weight: {sample_weight}\")\n",
    "        # Clear any previous fit\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "        epsilon = np.finfo(sample_weight.dtype).eps\n",
    "\n",
    "        zero_weight_mask = sample_weight == 0\n",
    "        for iboost in range(self.n_estimators):\n",
    "            # Avoid extremely small sample weight\n",
    "            sample_weight = np.clip(sample_weight, a_min=epsilon, a_max=None)\n",
    "            sample_weight[zero_weight_mask] = 0.0\n",
    "\n",
    "            # Boosting step\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(iboost, X, y, sample_weight)\n",
    "\n",
    "            # Early stopping\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "            if not np.isfinite(sample_weight_sum):\n",
    "                warnings.warn(\n",
    "                    (\n",
    "                        \"Sample weights have reached infinite values,\"\n",
    "                        f\" at iteration {iboost}, causing overflow. \"\n",
    "                        \"Iterations stopped. Try lowering the learning rate.\"\n",
    "                    ),\n",
    "                    stacklevel=2,\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize the sample weights\n",
    "                sample_weight /= sample_weight_sum\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _boost(self, iboost, X, y, sample_weight):\n",
    "        \"\"\"\n",
    "        Implement a single boost iteration.\n",
    "\n",
    "        Perform a single boost according to the AdaBoost.R2 algorithm and return the updated sample weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        iboost: int\n",
    "            The current boosting iteration.\n",
    "\n",
    "        X: array\n",
    "            The training input samples (length = 5)\n",
    "\n",
    "        y: array\n",
    "            The target values (real numbers)\n",
    "\n",
    "        sample_weight: array\n",
    "            The current sample weights.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample_weight: array\n",
    "            The updated sample weights.\n",
    "\n",
    "        estimator_weight: float\n",
    "            The weight of the estimator.\n",
    "\n",
    "        estimator_error: float\n",
    "            The error of the estimator.\n",
    "        \"\"\"\n",
    "        estimator = self.estimator\n",
    "\n",
    "        # Weighted sampling of the training data with replacement\n",
    "        boostrap_idx = np.random.choice(size=len(X), replace=True, p=sample_weight)\n",
    "\n",
    "        # Fit on the bootstrapped sample and obtain the predictions\n",
    "        estimator.fit(X, y)\n",
    "        y_pred = estimator.predict(X)\n",
    "\n",
    "        error_vect = np.abs(y_pred - y_true)\n",
    "        sample_mask = sample_weight > 0\n",
    "        masked_sample_weight = sample_weight[sample_mask]\n",
    "        masked_error_vector = error_vect[sample_mask]\n",
    "\n",
    "        error_max = np.max(error_vect[sample_mask])\n",
    "        if error_max != 0:\n",
    "            masked_error_vector /= error_max\n",
    "\n",
    "        if self.loss == \"square\":\n",
    "            masked_error_vector **= 2\n",
    "        elif self.loss == \"exponential\":\n",
    "            masked_error_vector = 1.0 - np.exp(-masked_error_vector)\n",
    "\n",
    "        # Calculate the average loss\n",
    "        estimator_error = (masked_sample_weight * masked_error_vector).sum()\n",
    "\n",
    "        if estimator_error <= 0:\n",
    "            # Stop if fit is perfect\n",
    "            return sample_weight, 1.0, 0.0\n",
    "        elif estimator_error >= 0.5:\n",
    "            # Discard current estimator only if it isn't the only one\n",
    "            if len(self.estimators_) > 1:\n",
    "                self.estimators_.pop(-1)\n",
    "            return None, None, None\n",
    "\n",
    "        beta = estimator_error / (1.0 - estimator_error)\n",
    "\n",
    "        # Boost weight using AdaBoost.R2 algo\n",
    "        estimator_weight = self.learning_rate * np.log(1.0 / beta)\n",
    "\n",
    "        if not iboost == self.n_estimators - 1:\n",
    "            # Update the sample weights\n",
    "            sample_weight[sample_mask] *= np.power(beta, 1.0 - masked_error_vector * self.learning_rate)\n",
    "\n",
    "    def _get_median_predict(self, X, limit):\n",
    "        # Evaluate predictions of all estimators (ensemble)\n",
    "        predictions = np.array([estimator.predict(X) for estimator in self.estimators_[:limit]])\n",
    "\n",
    "        # Sort the predictions\n",
    "        sorted_idx = np.argsort(predictions, axis=1)\n",
    "\n",
    "        # Find index of median prediction for each sample\n",
    "        weight_cdf = np.cumsum(self.estimator_weights_[:limit], axis=1)\n",
    "        median_or_above = weight_cdf >= 0.5\n",
    "        median_idx = np.argmax(median_or_above, axis=1)\n",
    "\n",
    "        median_estimators = sorted_idx[np.arange(len(X), median_idx)]\n",
    "\n",
    "        # Return the median prediction\n",
    "        return predictions[np.arange(len(X)), median_estimators]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: array\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        # Get the median prediction\n",
    "        return self._get_median_predict(X, len(self.estimators_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_1 = AdaboostRegressor(estimator=gpab, n_estimators=100, learning_rate=1.0, loss=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m get_X(gpab)\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m get_y(gpab)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mregr_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[125], line 56\u001b[0m, in \u001b[0;36mAdaboostRegressor.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    Build a boosted ensemble of estimators from the training set (X, y).\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     sample_weight \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintial sample weight: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_weight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Clear any previous fit\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "X = get_X(gpab)\n",
    "y = get_y(gpab)\n",
    "regr_1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69542                [teary, adler, tells, of, family]\n",
       "142293    [ethiopia, tigray, refugees, sudan, eritrea]\n",
       "188137           [vic, corruption, fighter, tells, of]\n",
       "123849                [fowler, fury, set, for, crisis]\n",
       "18781                 [love, pleads, guilty, to, drug]\n",
       "                              ...                     \n",
       "166473      [tendulkar, confident, ahead, of, special]\n",
       "265261               [russia, to, build, reactors, in]\n",
       "17271           [hotel, for, former, academy, cinemas]\n",
       "7032             [voss, out, fletcher, faces, nervous]\n",
       "52567       [abbott, backs, morrisons, asylum, seeker]\n",
       "Name: 0, Length: 2669, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpab.inputword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teary', 'adler', 'tells', 'of', 'family']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpab.inputword.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'protected_div(square(c), b)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(one_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function <lambda>(a, b, c, d, e)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_tree = gpab.pop[0]\n",
    "func = deap_gp.compile(one_tree, gpab.pset)\n",
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teary', 'adler', 'tells', 'of', 'family']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sentence = gpab.inputword.iloc[0]\n",
    "one_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([gpab.embeddings[char] for char in one_sentence])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.3991321e-01, -1.2687312e-02,  3.8116419e-01,  4.7464702e-02,\n",
       "       -6.3511804e-02, -3.0491473e-03,  6.4744306e-04,  1.7864481e+00,\n",
       "       -4.4006062e-01,  2.3497958e-03], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for words in gpab.inputword:\n",
    "    X = np.array([gpab.embeddings[char] for char in words])\n",
    "    y_pred = func(*X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669, 10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list = np.array([func(*np.array([gpab.embeddings[char] for char in words])) for words in gpab.inputword])\n",
    "y_pred_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([gpab.embeddings[char] for char in gpab.realword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669, 10)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_list = np.array(np.array([gpab.embeddings[char] for char in gpab.realword]))\n",
    "y_true_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpab.inputword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differences: (2669,)\n",
      "D: 11136956.0\n",
      "L_1: (2669,), L_2: (2669,), L_3: (2669,)\n",
      "L: (2669,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.04124756e-05, 1.67119513e-05, 1.08809853e-06, ...,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_weight(trees):\n",
    "    differences = np.zeros((len(trees.inputword)))\n",
    "    num_data = 0\n",
    "    for idx, tree in enumerate(trees.pop):\n",
    "        func = deap_gp.compile(tree, trees.pset)\n",
    "        # print(f\"tree: {tree}\")\n",
    "\n",
    "        y_preds_for_one_tree = np.array([func(*np.array([trees.embeddings[char] for char in words])) for words in trees.inputword])\n",
    "\n",
    "        y_trues_for_one_tree = np.array(np.array([trees.embeddings[char] for char in trees.realword]))\n",
    "\n",
    "        # Calculate the difference\n",
    "        differences_for_one_tree = np.linalg.norm((y_preds_for_one_tree - y_trues_for_one_tree), axis=1)\n",
    "        # print(f\"differences_for_one_tree: {differences_for_one_tree.shape}\")\n",
    "\n",
    "        differences[num_data] = differences_for_one_tree[0]\n",
    "\n",
    "        num_data += 1\n",
    "\n",
    "    print(f\"differences: {differences.shape}\")\n",
    "\n",
    "    # Find the supremum (maximum) of these differences\n",
    "    D = np.max(differences)\n",
    "    print(f\"D: {D}\")\n",
    "\n",
    "    L_1 = differences / D\n",
    "    L_2 = np.square(differences) / np.square(D)\n",
    "    L_3 = 1 - np.exp(-differences / D)\n",
    "    print(f\"L_1: {L_1.shape}, L_2: {L_2.shape}, L_3: {L_3.shape}\")\n",
    "    L = np.mean(np.stack((L_1, L_2, L_3), axis=0), axis=0)\n",
    "    print(f\"L: {L.shape}\")\n",
    "\n",
    "    return L\n",
    "\n",
    "result_L = update_weight(gpab)\n",
    "result_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = result_L / (1 - result_L)\n",
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"weights_update\"] = data[\"weights\"] * np.exp(beta * )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
