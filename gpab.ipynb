{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        algorithm,\n",
    "        embedding_type,\n",
    "        dimension,\n",
    "        population_size,\n",
    "        crossover_method,\n",
    "        cross_prob,\n",
    "        mut_prob,\n",
    "        num_generations,\n",
    "        num_evaluations,\n",
    "        debug,\n",
    "    ):\n",
    "        self.algorithm = algorithm\n",
    "        self.embedding_type = embedding_type\n",
    "        self.dimension = dimension\n",
    "        self.population_size = population_size\n",
    "        self.crossover_method = crossover_method\n",
    "        self.cross_prob = cross_prob\n",
    "        self.mut_prob = mut_prob\n",
    "        self.num_generations = num_generations\n",
    "        self.num_evaluations = num_evaluations\n",
    "        self.debug = debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = config(\"gpab\",\n",
    "        \"word2vec\",\n",
    "        10,\n",
    "        50,\n",
    "        \"cx_random\",\n",
    "        1,\n",
    "        0.1,\n",
    "        10,\n",
    "        1000,\n",
    "        False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "from deap import gp as deap_gp\n",
    "import gp\n",
    "from data import get_embeddings, get_testing_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# seed = 1126\n",
    "# random.seed(seed)\n",
    "\n",
    "# python main.py -algo \"gpab\" -e \"word2vec\" -n 10 -p 250 -c cx_random -pc 1 -pm 0.1 -g 100\n",
    "\n",
    "\n",
    "def update_weights(\n",
    "    gpab,\n",
    "    best_ind,\n",
    "    X,\n",
    "    y,\n",
    "    iboost,\n",
    "    sample_weight,\n",
    "    learning_rate,\n",
    "    loss,\n",
    "    ensemble,\n",
    "    num_ensemble,\n",
    "):\n",
    "    y_pred = gp.get_predict_vec(gpab, best_ind)\n",
    "\n",
    "    error_vect = np.linalg.norm(y - y_pred, axis=1)\n",
    "    sample_mask = sample_weight > 0\n",
    "    masked_sample_weight = sample_weight[sample_mask]\n",
    "    masked_error_vector = error_vect[sample_mask]\n",
    "\n",
    "    error_max = masked_error_vector.max()\n",
    "    if error_max != 0:\n",
    "        masked_error_vector /= error_max\n",
    "\n",
    "    if loss == \"square\":\n",
    "        masked_error_vector **= 2\n",
    "    elif loss == \"exponential\":\n",
    "        masked_error_vector = 1.0 - np.exp(-masked_error_vector)\n",
    "\n",
    "    # Culcalate the average loss\n",
    "    estimator_error = (masked_sample_weight * masked_error_vector).sum()\n",
    "    if estimator_error <= 0:\n",
    "        # Stop if fit is perfect\n",
    "        return sample_weight, 1.0, 0.0\n",
    "    elif estimator_error >= 0.5:\n",
    "        # Discard the estimator if worse than random guessing and it isn't the only one\n",
    "        if len(ensemble) > 0:\n",
    "            ensemble.pop(-1)\n",
    "        return None, None, None\n",
    "\n",
    "    beta = estimator_error / (1.0 - estimator_error)\n",
    "\n",
    "    # Boost weight using AdaBoost.R2 algorithm\n",
    "    estimator_weight = learning_rate * np.log(1.0 / beta)\n",
    "\n",
    "    if not iboost == num_ensemble - 1:\n",
    "        sample_weight[sample_mask] *= np.power(\n",
    "            beta, (1.0 - masked_error_vector) * learning_rate\n",
    "        )\n",
    "\n",
    "    return sample_weight, estimator_weight, estimator_error\n",
    "\n",
    "\n",
    "def boosting(\n",
    "    gpab, data, ensemble, num_ensemble, iboost, sample_weight, loss, learning_rate\n",
    "):\n",
    "    epsilon = np.finfo(sample_weight.dtype).eps\n",
    "    zero_weight_mask = sample_weight == 0.0\n",
    "\n",
    "    # Get the best individual\n",
    "    best_ind = max(gpab.pop, key=lambda x: x.fitness.values)\n",
    "    # Avoid extremely small weights\n",
    "    sample_weight = np.clip(sample_weight, a_min=epsilon, a_max=None)\n",
    "    sample_weight[zero_weight_mask] = 0.0\n",
    "\n",
    "    # Boosting step\n",
    "    X = gp.get_X(gpab)\n",
    "    y = gp.get_y(gpab)\n",
    "    sample_weight, estimator_weight, estimator_error = update_weights(\n",
    "        gpab,\n",
    "        best_ind,\n",
    "        X,\n",
    "        y,\n",
    "        iboost,\n",
    "        sample_weight,\n",
    "        learning_rate,\n",
    "        loss,\n",
    "        ensemble,\n",
    "        num_ensemble,\n",
    "    )\n",
    "\n",
    "    # Early termination\n",
    "    if sample_weight is None:\n",
    "        return sample_weight, best_ind\n",
    "\n",
    "    # Stop if error is zero\n",
    "    if estimator_error == 0:\n",
    "        return sample_weight, best_ind\n",
    "\n",
    "    sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "    if not np.isfinite(sample_weight_sum):\n",
    "        warnings.warn(\n",
    "            (\n",
    "                \"Sample weights have reached infinite values,\"\n",
    "                f\" at iteration {iboost}, causing overflow. \"\n",
    "                \"Iterations stopped. Try lowering the learning rate.\"\n",
    "            ),\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        return sample_weight, best_ind\n",
    "\n",
    "    # Stop if the sum of sample weights has become non-positive\n",
    "    if sample_weight_sum <= 0:\n",
    "        return sample_weight, best_ind\n",
    "\n",
    "    if iboost < num_ensemble - 1:\n",
    "        # Normalize\n",
    "        sample_weight /= sample_weight_sum\n",
    "\n",
    "    print(\"Sample weight: \", sample_weight)\n",
    "    data[\"weights_update\"] *= sample_weight\n",
    "\n",
    "    return sample_weight, best_ind\n",
    "\n",
    "\n",
    "def select_new_data(gpab, best, data):\n",
    "    # Update the population dataset\n",
    "    select_new_data = np.random.uniform(0, 1, len(data))\n",
    "    data[\"cumulative_weights\"] = data[\"weights_update\"].cumsum()\n",
    "\n",
    "    # Find the indices of the closest rows in cumulative_weights for each value in select_new_data\n",
    "    indices = np.digitize(select_new_data, data[\"cumulative_weights\"])\n",
    "\n",
    "    # Ensure indices are within bounds\n",
    "    indices = np.clip(indices, 0, len(data) - 1)\n",
    "\n",
    "    # Create new dataset by selecting rows from original dataset based on indices\n",
    "    new_dataset = data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "    gpab.data = new_dataset\n",
    "    # Evaluate the entire population\n",
    "    fitnesses = map(gpab.toolbox.evaluate, gpab.pop)\n",
    "    for ind, fit in zip(gpab.pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "\n",
    "def evolving(\n",
    "    gpab, data, ensemble, num_ensemble, iboost, sample_weight, loss, learning_rate\n",
    "):\n",
    "    while gpab.n_gen < gpab.max_gen:\n",
    "        gpab.select()\n",
    "\n",
    "        # Boosting\n",
    "        if gpab.n_gen % iboost == 0:\n",
    "            sample_weight, best = boosting(\n",
    "                gpab,\n",
    "                data,\n",
    "                ensemble,\n",
    "                num_ensemble,\n",
    "                iboost,\n",
    "                sample_weight,\n",
    "                loss,\n",
    "                learning_rate,\n",
    "            )\n",
    "            ensemble.append(best)\n",
    "\n",
    "            # Update the data\n",
    "            select_new_data(gpab, best, data)\n",
    "\n",
    "        gpab.n_gen += 1\n",
    "\n",
    "\n",
    "def run_trail(Config):\n",
    "    data, embeddings, embedding_model = get_embeddings(\n",
    "        Config.embedding_type, Config.dimension\n",
    "    )\n",
    "\n",
    "    cx_method = gp.get_cx_num(Config.crossover_method)\n",
    "\n",
    "    # Initialize instance weights\n",
    "    data[\"weights\"] = 1.0 / len(data)\n",
    "    data[\"weights_update\"] = 1.0 / len(data)\n",
    "\n",
    "    ensemble = []  # Ensemble to store the best individuals\n",
    "    iboost = 1  # Config.num_generations / 10  # Boosting interval\n",
    "    num_ensemble = 5\n",
    "    loss = \"linear\"\n",
    "    learning_rate = 1.0\n",
    "    sample_weight = np.array(data[\"weights_update\"])\n",
    "\n",
    "    gpab = gp.GP(\n",
    "        Config.algorithm,\n",
    "        Config.embedding_type,\n",
    "        Config.dimension,\n",
    "        Config.population_size,\n",
    "        cx_method,\n",
    "        Config.cross_prob,\n",
    "        Config.mut_prob,\n",
    "        Config.num_generations,\n",
    "        Config.num_evaluations,\n",
    "        data,\n",
    "        embeddings,\n",
    "        Config.run,\n",
    "    )\n",
    "    gpab.initialize_pop()\n",
    "    evolving(\n",
    "        gpab, data, ensemble, num_ensemble, iboost, sample_weight, loss, learning_rate\n",
    "    )\n",
    "\n",
    "    print(\"Starting testing...\")\n",
    "    print(f\"Ensemble: {ensemble}\")\n",
    "\n",
    "\n",
    "    # print(f\"Archive: {ensemble}\")\n",
    "    # os.makedirs(f\"archive/{Config.algorithm}/result/\", exist_ok=True)\n",
    "    # file_name = \"archive/result.archive.\" + gpab.csv_name()\n",
    "    # with open(f\"archive/{Config.algorithm}/result/{file_name}.txt\", \"w\") as f:\n",
    "    #     for idx, top in enumerate(ensemble):\n",
    "    #         f.write(f\"Forest {idx}\\n\")\n",
    "    #         for tree in top:\n",
    "    #             f.write(f\"{tree}\\n\")\n",
    "    #         f.write(\"\\n\")\n",
    "    return ensemble\n",
    "\n",
    "def gpab(config):\n",
    "    for i in range(1):\n",
    "        config.run = i + 1\n",
    "        ensemble = run_trail(config)\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample weight:  [0.00038824 0.00039922 0.00037763 ... 0.00039207 0.0003746  0.00035964]\n",
      "Sample weight:  [0.00040024 0.00042247 0.0003793  ... 0.00040793 0.00037343 0.00034508]\n",
      "Sample weight:  [0.0004107  0.00044426 0.00037986 ... 0.00042223 0.00037135 0.0003311 ]\n",
      "Sample weight:  [0.00041971 0.00046451 0.00037947 ... 0.00043498 0.00036852 0.00031778]\n",
      "Sample weight:  [0.00042735 0.00048315 0.00037827 ... 0.00044623 0.00036511 0.00030519]\n",
      "Sample weight:  [0.00043372 0.00050018 0.00037643 ... 0.00045606 0.00036126 0.00029335]\n",
      "Sample weight:  [0.00043895 0.00051559 0.00037408 ... 0.00046455 0.0003571  0.0002823 ]\n",
      "Sample weight:  [0.00044316 0.00052946 0.00037133 ... 0.00047182 0.00035274 0.00027202]\n",
      "Sample weight:  [0.00044647 0.00054185 0.00036832 ... 0.00047798 0.0003483  0.00026251]\n",
      "Sample weight:  [0.000449   0.00055284 0.00036513 ... 0.00048314 0.00034385 0.00025374]\n",
      "Starting testing...\n",
      "Ensemble: [[<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>], [<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>], [<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>], [<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>], [<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>], [<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>], [<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>], [<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>], [<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>], [<deap.gp.Primitive object at 0x7f72f2398900>, <deap.gp.Terminal object at 0x7f72ae3c6700>, <deap.gp.Terminal object at 0x7f72ae3c6840>]]\n"
     ]
    }
   ],
   "source": [
    "ensemble = gpab(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'deap.gp' has no attribute 'GP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensemble_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EC/EC-term-paper/gp.py:519\u001b[0m, in \u001b[0;36mensemble_testing\u001b[0;34m(ensemble, Config)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mensemble_testing\u001b[39m(ensemble, Config, embedding_model):\n\u001b[1;32m    515\u001b[0m     test_data, test_embeddings \u001b[38;5;241m=\u001b[39m get_testing_dataset(\n\u001b[1;32m    516\u001b[0m         Config\u001b[38;5;241m.\u001b[39membedding_type, Config\u001b[38;5;241m.\u001b[39mdimension\n\u001b[1;32m    517\u001b[0m     )\n\u001b[0;32m--> 519\u001b[0m     tree \u001b[38;5;241m=\u001b[39m gp\u001b[38;5;241m.\u001b[39mGP(\n\u001b[1;32m    520\u001b[0m         Config\u001b[38;5;241m.\u001b[39malgorithm,\n\u001b[1;32m    521\u001b[0m         Config\u001b[38;5;241m.\u001b[39membedding_type,\n\u001b[1;32m    522\u001b[0m         Config\u001b[38;5;241m.\u001b[39mdimension,\n\u001b[1;32m    523\u001b[0m         Config\u001b[38;5;241m.\u001b[39mpopulation_size,\n\u001b[1;32m    524\u001b[0m         get_cx_num(Config\u001b[38;5;241m.\u001b[39mcrossover_method),\n\u001b[1;32m    525\u001b[0m         Config\u001b[38;5;241m.\u001b[39mcross_prob,\n\u001b[1;32m    526\u001b[0m         Config\u001b[38;5;241m.\u001b[39mmut_prob,\n\u001b[1;32m    527\u001b[0m         Config\u001b[38;5;241m.\u001b[39mnum_generations,\n\u001b[1;32m    528\u001b[0m         Config\u001b[38;5;241m.\u001b[39mnum_evaluations,\n\u001b[1;32m    529\u001b[0m         test_data,\n\u001b[1;32m    530\u001b[0m         test_embeddings,\n\u001b[1;32m    531\u001b[0m         Config\u001b[38;5;241m.\u001b[39mrun,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    533\u001b[0m     tree\u001b[38;5;241m.\u001b[39mregister()\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# Get the best 5 individuals\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deap.gp' has no attribute 'GP'"
     ]
    }
   ],
   "source": [
    "gp.ensemble_testing(ensemble, Config, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/home/nlplab/atwolin/EC/yes/envs/env_p/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/nlplab/atwolin/EC/yes/envs/env_p/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "# Get test data\n",
    "test_data, test_embeddings = get_testing_dataset(\n",
    "    Config.embedding_type, Config.dimension\n",
    ")\n",
    "# gpab.data = test_data\n",
    "# gpab.inputword = test_data[0].str.split(\" \").apply(lambda x: x[:5])\n",
    "# gpab.realword = test_data[0].str.split(\" \").str.get(5)\n",
    "# gpab.embeddings = test_embeddings\n",
    "\n",
    "gpab = gp.GP(\n",
    "        Config.algorithm,\n",
    "        Config.embedding_type,\n",
    "        Config.dimension,\n",
    "        Config.population_size,\n",
    "        cx_method,\n",
    "        Config.cross_prob,\n",
    "        Config.mut_prob,\n",
    "        Config.num_generations,\n",
    "        Config.num_evaluations,\n",
    "        test_data,\n",
    "        test_embeddings,\n",
    "        Config.run,\n",
    "    )\n",
    "gpab.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  [[<deap.gp.Primitive object at 0x7f7f952a6400>, <deap.gp.Terminal object at 0x7f7f95c12a00>, <deap.gp.Terminal object at 0x7f7f95c128c0>], [<deap.gp.Primitive object at 0x7f7f952a6400>, <deap.gp.Terminal object at 0x7f7f95c12a00>, <deap.gp.Terminal object at 0x7f7f95c128c0>], [<deap.gp.Primitive object at 0x7f7f952a6400>, <deap.gp.Terminal object at 0x7f7f95c12a00>, <deap.gp.Terminal object at 0x7f7f95c128c0>], [<deap.gp.Primitive object at 0x7f7f952a6400>, <deap.gp.Terminal object at 0x7f7f95c12a00>, <deap.gp.Terminal object at 0x7f7f95c128c0>], [<deap.gp.Primitive object at 0x7f7f952a6400>, <deap.gp.Terminal object at 0x7f7f95c12a00>, <deap.gp.Terminal object at 0x7f7f95c128c0>]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Get the best individuals\n",
    "archive = sorted(ensemble, key=lambda x: x.fitness.values, reverse=True)[:5]\n",
    "print(\"Archive: \", archive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gp.get_X(gpab)\n",
    "y = gp.get_y(gpab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10), (5, 10000))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average the predicted vectors\n",
    "y_pred_ensemble = np.zeros((len(archive), len(test_data), gpab.dim))\n",
    "archive_fitness_for_each_datum = np.zeros((len(archive), len(test_data)))\n",
    "# print(f\"y_pred_ensemble: {y_pred_ensemble.shape}\")\n",
    "for idx, tree in enumerate(archive):\n",
    "    # Get predict vecoters of all sentences\n",
    "    y_pred = gp.get_predict_vec(gpab, tree)\n",
    "    # print(f\"y_pred: {y_pred.shape}\")\n",
    "    one_tree_fitness = cosine_similarity(y_pred, y).diagonal()\n",
    "    # print(f\"fitness: {one_tree_fitness.shape}\")\n",
    "    archive_fitness_for_each_datum[idx] = one_tree_fitness\n",
    "    y_pred_ensemble[idx] = y_pred\n",
    "avg_y_pred = np.mean(y_pred_ensemble, axis=0)\n",
    "avg_y_pred.shape, archive_fitness_for_each_datum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness_avg = cosine_similarity(avg_y_pred, y).diagonal()\n",
    "fitness_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted words of all sentences\n",
    "words = []\n",
    "for vec in avg_y_pred:\n",
    "    word = gp.get_predict_word(vec, Config.embedding_type, embedding_model)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentences, predicted words, and record\n",
    "csv_name = \"result.\" + gpab.csv_name()\n",
    "\n",
    "os.makedirs(f\"archive/{Config.algorithm}/result/\", exist_ok=True)\n",
    "with open(f\"archive/{Config.algorithm}/result/{csv_name}\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"input_word\",\n",
    "            \"predict_word\",\n",
    "            \"real_word\",\n",
    "            \"fitness value[0]\",\n",
    "            \"fitness value[1]\",\n",
    "            \"fitness value[2]\",\n",
    "            \"fitness value[3]\",\n",
    "            \"fitness value[4]\",\n",
    "            \"avg fiteness value\",\n",
    "            \"tree[0]\",\n",
    "            \"tree[1]\",\n",
    "            \"tree[2]\",\n",
    "            \"tree[3]\",\n",
    "            \"tree[4]\",\n",
    "        ]\n",
    "    )\n",
    "    for i in range(len(test_data)):\n",
    "        row = [\n",
    "            str(gpab.inputword[i]),\n",
    "            words[i],\n",
    "            gpab.realword[i],\n",
    "            archive_fitness_for_each_datum[0][i],\n",
    "            archive_fitness_for_each_datum[1][i],\n",
    "            archive_fitness_for_each_datum[2][i],\n",
    "            archive_fitness_for_each_datum[3][i],\n",
    "            archive_fitness_for_each_datum[4][i],\n",
    "            fitness_avg[i],\n",
    "            str(archive[0]),\n",
    "            str(archive[1]),\n",
    "            str(archive[2]),\n",
    "            str(archive[3]),\n",
    "            str(archive[4]),\n",
    "        ]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deap import gp as deap_gp\n",
    "import gp\n",
    "from data import get_embeddings, get_testing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1126\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "data, embeddings, embedding_model = get_embeddings(\n",
    "        Config.embedding_type, Config.dimension\n",
    "    )\n",
    "\n",
    "cx_method = gp.get_cx_num(Config.crossover_method)\n",
    "\n",
    "# Initialize instance weights\n",
    "data[\"weights\"] = 1.0 / len(data)\n",
    "data[\"weights_update\"] = 1.0 / len(data)\n",
    "\n",
    "iboost = 10  # Boosting interval\n",
    "ensemble = []  # Ensemble to store the best individuals\n",
    "loss = \"linear\"\n",
    "learning_rate = 1.0\n",
    "sample_weight = np.array(data[\"weights_update\"])\n",
    "num_ensemble = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "test_data, test_embeddings = get_testing_dataset(Config.embedding_type, Config.dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fourth ashes test day three wrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>council airs tafe funding cut worries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miners praised for fatigue management efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>costello quiet on coles takeover prospects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i didnt axe katich says clarke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>give bartlett a chance rudd says</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>nature play documentary into outdoor play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>hospital mental health unit revamp overdue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>brindabella fire plan open for comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>police urge residents to report crime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0\n",
       "0                  fourth ashes test day three wrap\n",
       "1             council airs tafe funding cut worries\n",
       "2     miners praised for fatigue management efforts\n",
       "3        costello quiet on coles takeover prospects\n",
       "4                    i didnt axe katich says clarke\n",
       "...                                             ...\n",
       "9995               give bartlett a chance rudd says\n",
       "9996      nature play documentary into outdoor play\n",
       "9997     hospital mental health unit revamp overdue\n",
       "9998         brindabella fire plan open for comment\n",
       "9999          police urge residents to report crime\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/home/nlplab/atwolin/EC/yes/envs/env_p/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/nlplab/atwolin/EC/yes/envs/env_p/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Get test data\n",
    "test_data, test_embeddings = get_testing_dataset(\n",
    "    Config.embedding_type, Config.dimension\n",
    ")\n",
    "gpab.data = test_data\n",
    "gpab.embeddings = test_embeddings\n",
    "\n",
    "# Get the best individuals\n",
    "archive = sorted(ensemble, key=lambda x: x.fitness.values, reverse=True)[:5]\n",
    "\n",
    "# Average the predicted vectors\n",
    "y_pred_ensemble = np.zeros(len(archive))\n",
    "for idx, tree in enumerate(archive):\n",
    "    # Get predict vecoters of all sentences\n",
    "    y_pred = gp.get_predict_vec(gpab, tree)\n",
    "    y_pred_ensemble[idx] = y_pred\n",
    "avg_y_pred = np.mean(y_pred_ensemble, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.append(best_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<deap.gp.Primitive at 0x7f8014063900>,\n",
       "  <deap.gp.Terminal at 0x7f8014007400>,\n",
       "  <deap.gp.Terminal at 0x7f8014007240>]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind = max(gpab.pop, key=lambda x: x.fitness.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre_list = gp.get_predict_vec(gpab, best_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add(a, e)'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(best_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vec = get_predict(gpab, best_ind)\n",
    "e_type = Config.embedding_type\n",
    "e_model = embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in y_pred_vec:\n",
    "    print(f\"vec: {vec}\")\n",
    "    word = get_predict_word(vec, e_type, e_model)\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>weights</th>\n",
       "      <th>weights_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139785</th>\n",
       "      <td>awards recognise road workers cyclone efforts</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33611</th>\n",
       "      <td>evil paedophile alfred impicciatore challenges...</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126496</th>\n",
       "      <td>cambodian opposition leader barred from elections</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10730</th>\n",
       "      <td>school community garden helps bushfire affected</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145672</th>\n",
       "      <td>gold diggers excited by tennant creek</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229746</th>\n",
       "      <td>church urged to protect heritage values</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141959</th>\n",
       "      <td>qld govt to revamp school curriculum</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173368</th>\n",
       "      <td>education budget cuts not ruled out</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161730</th>\n",
       "      <td>pair arrested over melbourne cannabis farm</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137077</th>\n",
       "      <td>family first prepared for early election</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2669 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0   weights  \\\n",
       "139785      awards recognise road workers cyclone efforts  0.000375   \n",
       "33611   evil paedophile alfred impicciatore challenges...  0.000375   \n",
       "126496  cambodian opposition leader barred from elections  0.000375   \n",
       "10730     school community garden helps bushfire affected  0.000375   \n",
       "145672              gold diggers excited by tennant creek  0.000375   \n",
       "...                                                   ...       ...   \n",
       "229746            church urged to protect heritage values  0.000375   \n",
       "141959               qld govt to revamp school curriculum  0.000375   \n",
       "173368                education budget cuts not ruled out  0.000375   \n",
       "161730         pair arrested over melbourne cannabis farm  0.000375   \n",
       "137077           family first prepared for early election  0.000375   \n",
       "\n",
       "        weights_update  \n",
       "139785        0.000375  \n",
       "33611         0.000375  \n",
       "126496        0.000375  \n",
       "10730         0.000375  \n",
       "145672        0.000375  \n",
       "...                ...  \n",
       "229746        0.000375  \n",
       "141959        0.000375  \n",
       "173368        0.000375  \n",
       "161730        0.000375  \n",
       "137077        0.000375  \n",
       "\n",
       "[2669 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpab = gp.GP(\n",
    "    Config.algorithm,\n",
    "    Config.embedding_type,\n",
    "    Config.dimension,\n",
    "    Config.population_size,\n",
    "    cx_method,\n",
    "    Config.cross_prob,\n",
    "    Config.mut_prob,\n",
    "    Config.num_generations,\n",
    "    Config.num_evaluations,\n",
    "    data,\n",
    "    embeddings,\n",
    "    run=1\n",
    ")\n",
    "gpab.initialize_pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(trees):\n",
    "        X_list = np.array([np.array([trees.embeddings[char] for char in words]) for words in trees.inputword])\n",
    "        return X_list\n",
    "\n",
    "def get_predict(trees, individual):\n",
    "        func = deap_gp.compile(individual, gpab.pset)\n",
    "        y_pred_list = np.array([func(*np.array([trees.embeddings[char] for char in words])) for words in trees.inputword])\n",
    "        return y_pred_list\n",
    "\n",
    "def get_y(trees):\n",
    "    y_true_list = np.array(np.array([trees.embeddings[char] for char in trees.realword]))\n",
    "    return y_true_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "iboost = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample weight:  [0.00042234 0.00035713 0.00043287 ... 0.0004323  0.00047308 0.00042408]\n",
      "Generation:  0\n",
      "Generation:  1\n"
     ]
    }
   ],
   "source": [
    "gpab.n_gen = 0\n",
    "# while gpab.n_gen < gpab.max_gen:\n",
    "while gpab.n_gen < 2:\n",
    "    gpab.select()\n",
    "\n",
    "    epsilon = np.finfo(sample_weight.dtype).eps\n",
    "    zero_weight_mask = sample_weight == 0.0\n",
    "\n",
    "    # Boosting\n",
    "    if gpab.n_gen % iboost == 0:\n",
    "        # for iboost in range(len(num_ensemble)):\n",
    "\n",
    "        # Get the best individual\n",
    "        best_ind = max(gpab.pop, key=lambda x: x.fitness.values)\n",
    "        # Avoid extremely small weights\n",
    "        sample_weight = np.clip(sample_weight, a_min=epsilon, a_max=None)\n",
    "        sample_weight[zero_weight_mask] = 0.0\n",
    "\n",
    "        # Boosting step\n",
    "        X = get_X(gpab)\n",
    "        y = get_y(gpab)\n",
    "        sample_weight, estimator_weight, estimator_error = boosting(iboost, gpab, best_ind, X, y, sample_weight, learning_rate, loss)\n",
    "\n",
    "        # Early termination\n",
    "        if sample_weight is None:\n",
    "            break\n",
    "\n",
    "        # Stop if error is zero\n",
    "        if estimator_error == 0:\n",
    "            break\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "        if not np.isfinite(sample_weight_sum):\n",
    "            warnings.warn(\n",
    "                (\n",
    "                    \"Sample weights have reached infinite values,\"\n",
    "                    f\" at iteration {iboost}, causing overflow. \"\n",
    "                    \"Iterations stopped. Try lowering the learning rate.\"\n",
    "                ),\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            break\n",
    "\n",
    "        # Stop if the sum of sample weights has become non-positive\n",
    "        if sample_weight_sum <= 0:\n",
    "            break\n",
    "\n",
    "        if iboost < num_ensemble - 1:\n",
    "            # Normalize\n",
    "            sample_weight /= sample_weight_sum\n",
    "\n",
    "        print(\"Sample weight: \", sample_weight)\n",
    "\n",
    "        data[\"weights_update\"] *= sample_weight\n",
    "\n",
    "        # Update the population dataset\n",
    "        select_new_data = np.random.uniform(0, 1, len(data))\n",
    "        data[\"cumulative_weights\"] = data[\"weights_update\"].cumsum()\n",
    "        # Find the indices of the closest rows in cumulative_weights for each value in select_new_data\n",
    "        indices = np.digitize(select_new_data, data[\"cumulative_weights\"])\n",
    "        # Create new dataset by selecting rows from original dataset based on indices\n",
    "        new_dataset = data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "        gpab.data = new_dataset\n",
    "        # Evaluate the entire population\n",
    "        fitnesses = map(gpab.toolbox.evaluate, gpab.pop)\n",
    "        for ind, fit in zip(gpab.pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "    print(\"Generation: \", gpab.n_gen)\n",
    "    gpab.n_gen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"weights_update\"] *= sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00037467, 0.00037467, 0.00037467, ..., 0.00037467, 0.00037467,\n",
       "       0.00037467])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind = max(gpab.pop, key=lambda x: x.fitness.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4588336239329494,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ind.fitness.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting(iboost, gpab, best_ind, X, y, sample_weight, learning_rate, loss):\n",
    "    y_pred = get_predict(gpab, best_ind)\n",
    "\n",
    "    error_vect = np.linalg.norm(y - y_pred, axis=1)\n",
    "    sample_mask = sample_weight > 0\n",
    "    masked_sample_weight = sample_weight[sample_mask]\n",
    "    masked_error_vector = error_vect[sample_mask]\n",
    "\n",
    "    error_max = masked_error_vector.max()\n",
    "    if error_max != 0:\n",
    "        masked_error_vector /= error_max\n",
    "\n",
    "    if loss == \"square\":\n",
    "        masked_error_vector **= 2\n",
    "    elif loss == \"exponential\":\n",
    "        masked_error_vector = 1.0 - np.exp(-masked_error_vector)\n",
    "\n",
    "    # Culcalate the average loss\n",
    "    estimator_error = (masked_sample_weight * masked_error_vector).sum()\n",
    "    if estimator_error <= 0:\n",
    "        # Stop if fit is perfect\n",
    "        return sample_weight, 1.0, 0.0\n",
    "    elif estimator_error >= 0.5:\n",
    "        # Discard the estimator if worse than random guessing and it isn't the only one\n",
    "        if len(ensemble) > 0:\n",
    "            ensemble.pop(-1)\n",
    "        return None, None, None\n",
    "\n",
    "    beta = estimator_error / (1.0 - estimator_error)\n",
    "\n",
    "    # Boost weight using AdaBoost.R2 algorithm\n",
    "    estimator_weight = learning_rate * np.log(1.0 / beta)\n",
    "\n",
    "    if not iboost == num_ensemble - 1:\n",
    "        sample_weight[sample_mask] *= np.power(\n",
    "        beta, (1.0 - masked_error_vector) * learning_rate\n",
    "    )\n",
    "\n",
    "    return sample_weight, estimator_weight, estimator_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sample_weight = np.array(data[\"weights_update\"])\n",
    "sample_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_predict(gpab)\n",
    "y = get_y(gpab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 1\n",
    "error_vect = np.linalg.norm(y - y_pred, axis=1)\n",
    "error_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 2\n",
    "sample_mask = sample_weight > 0\n",
    "sample_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 3\n",
    "masked_sample_weight = sample_weight[sample_mask]\n",
    "masked_sample_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 4\n",
    "masked_error_vector = error_vect[sample_mask]\n",
    "masked_error_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 5\n",
    "error_max = masked_error_vector.max()\n",
    "error_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082835.4"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8427035e-05, 4.3844911e-06, 1.7800233e-04, ..., 1.4145022e-03,\n",
       "       6.5471912e-05, 9.6160511e-06], dtype=float32)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 6\n",
    "if error_max != 0:\n",
    "    masked_error_vector /= error_max\n",
    "masked_error_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line 7\n",
    "if loss == \"square\":\n",
    "    masked_error_vector **= 2\n",
    "elif loss == \"exponential\":\n",
    "    masked_error_vector = 1.0 - np.exp(-masked_error_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006279703240808366"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 8\n",
    "estimator_error = (masked_sample_weight * masked_error_vector).sum()\n",
    "estimator_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (2712928142.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[307], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    return sample_weight\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# line 9\n",
    "# Calculate the average loss\n",
    "if estimator_error <= 0:\n",
    "    # Stop if fit is perfect\n",
    "    return sample_weight\n",
    "elif estimator_error >= 0.5:\n",
    "    # Discard the estimator if worse than random guessing and it isn't the only one\n",
    "    if len(ensemble) > 0:\n",
    "        ensemble.pop(-1)\n",
    "    return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006283649186024124"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 10\n",
    "beta = estimator_error / (1.0 - estimator_error)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.372389479680931"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 11\n",
    "estimator_weight = learning_rate * np.log(1.0 / beta)\n",
    "estimator_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line 12\n",
    "if not iboost == len(ensemble) - 1:\n",
    "    sample_weight[sample_mask] *= np.power(\n",
    "        beta, (1.0 - masked_error_vector) * learning_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line 13\n",
    "return sample_weight, estimator_weight, estimator_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPABRegressor():\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: deap.gp object\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "\n",
    "    n_estimators: int\n",
    "        The number of estimators to train, a.k.a. the number of population of GP trees.\n",
    "\n",
    "    learning_rate: float, default=1.0\n",
    "        The learning rate of the boosting algorithm.\n",
    "\n",
    "    loss: {'linear', 'square', 'exponential'}, optional\n",
    "        The loss function to use when updating the weights after each iteration.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimators_: estimator\n",
    "        The base estimator from which the ensemble is grown.\n",
    "\n",
    "    estimators_: list of regressors\n",
    "        The collection of fitted sub-estimators.\n",
    "\n",
    "    estimator_weights_: array-like of shape (n_estimators,)\n",
    "        Weights for each estimator in the boosted ensemble.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, n_estimators, learning_rate=1, loss=\"linear\"):\n",
    "        self.estimator = estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Build a boosted ensemble of estimators from the training set (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array\n",
    "            The training input samples (length = 5)\n",
    "\n",
    "        y: array\n",
    "            The target values (real numbers)\n",
    "\n",
    "        sample_weight: array\n",
    "            The sample weights. If None, the sample weights are initialized to 1 / n_samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # sample_weight /= sample_weight.sum()\n",
    "\n",
    "        # Clear any previous fit\n",
    "        # self.estimators_ = []\n",
    "        # self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        # self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "        epsilon = np.finfo(sample_weight).eps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        zero_weight_mask = sample_weight == 0\n",
    "        for iboost in range(self.n_estimators):\n",
    "\n",
    "            # Avoid extremely small sample weight\n",
    "            sample_weight = np.clip(sample_weight, a_min=epsilon, a_max=None)\n",
    "            sample_weight[zero_weight_mask] = 0.0\n",
    "\n",
    "            # Boosting step\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(iboost, X, y, sample_weight)\n",
    "\n",
    "            # Early stopping\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "            if not np.isfinite(sample_weight_sum):\n",
    "                warnings.warn(\n",
    "                    (\n",
    "                        \"Sample weights have reached infinite values,\"\n",
    "                        f\" at iteration {iboost}, causing overflow. \"\n",
    "                        \"Iterations stopped. Try lowering the learning rate.\"\n",
    "                    ),\n",
    "                    stacklevel=2,\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize the sample weights\n",
    "                sample_weight /= sample_weight_sum\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _boost(self, iboost, X, y, sample_weight):\n",
    "        \"\"\"\n",
    "        Implement a single boost iteration.\n",
    "\n",
    "        Perform a single boost according to the AdaBoost.R2 algorithm and return the updated sample weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        iboost: int\n",
    "            The current boosting iteration.\n",
    "\n",
    "        X: array\n",
    "            The training input samples (length = 5)\n",
    "\n",
    "        y: array\n",
    "            The target values (real numbers)\n",
    "\n",
    "        sample_weight: array\n",
    "            The current sample weights.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample_weight: array\n",
    "            The updated sample weights.\n",
    "\n",
    "        estimator_weight: float\n",
    "            The weight of the estimator.\n",
    "\n",
    "        estimator_error: float\n",
    "            The error of the estimator.\n",
    "        \"\"\"\n",
    "        estimator = self.estimator\n",
    "\n",
    "        # Weighted sampling of the training data with replacement\n",
    "        # boostrap_idx = np.random.choice(random_state, size=len(X), replace=True, p=sample_weight)\n",
    "\n",
    "\n",
    "        # Fit on the bootstrapped sample and obtain the predictions\n",
    "        # estimator.fit(X, y)\n",
    "        # estimator.select()\n",
    "        y_pred = self.predict(estimator)\n",
    "\n",
    "        error_vect = np.abs(y_pred - y)\n",
    "        sample_mask = sample_weight > 0\n",
    "        masked_sample_weight = sample_weight[sample_mask]\n",
    "        masked_error_vector = error_vect[sample_mask]\n",
    "\n",
    "        error_max = np.max(error_vect[sample_mask])\n",
    "        if error_max != 0:\n",
    "            masked_error_vector /= error_max\n",
    "\n",
    "        if self.loss == \"square\":\n",
    "            masked_error_vector **= 2\n",
    "        elif self.loss == \"exponential\":\n",
    "            masked_error_vector = 1.0 - np.exp(-masked_error_vector)\n",
    "\n",
    "        # Calculate the average loss\n",
    "        estimator_error = (masked_sample_weight * masked_error_vector).sum()\n",
    "\n",
    "        if estimator_error <= 0:\n",
    "            # Stop if fit is perfect\n",
    "            return sample_weight, 1.0, 0.0\n",
    "        elif estimator_error >= 0.5:\n",
    "            # Discard current estimator only if it isn't the only one\n",
    "            if len(self.estimators_) > 1:\n",
    "                self.estimators_.pop(-1)\n",
    "            return None, None, None\n",
    "\n",
    "        beta = estimator_error / (1.0 - estimator_error)\n",
    "\n",
    "        # Boost weight using AdaBoost.R2 algo\n",
    "        estimator_weight = self.learning_rate * np.log(1.0 / beta)\n",
    "\n",
    "        if not iboost == self.n_estimators - 1:\n",
    "            # Update the sample weights\n",
    "            sample_weight[sample_mask] *= np.power(beta, 1.0 - masked_error_vector * self.learning_rate)\n",
    "\n",
    "    # def _get_median_predict(self, X, limit):\n",
    "        # Evaluate predictions of all estimators (ensemble)\n",
    "        # predictions = np.array([estimator.predict(X) for estimator in self.estimators_[:limit]])\n",
    "        # predictions = np.array([func(*np.array([gpab.embeddings[char] for char in words])) for words in gpab.inputword])\n",
    "\n",
    "        # # Sort the predictions\n",
    "        # sorted_idx = np.argsort(predictions, axis=1)\n",
    "\n",
    "        # # Find index of median prediction for each sample\n",
    "        # weight_cdf = np.cumsum(self.estimator_weights_[:limit], axis=1)\n",
    "        # median_or_above = weight_cdf >= 0.5\n",
    "        # median_idx = np.argmax(median_or_above, axis=1)\n",
    "\n",
    "        # median_estimators = sorted_idx[np.arange(len(X), median_idx)]\n",
    "\n",
    "        # # Return the median prediction\n",
    "        # return predictions[np.arange(len(X)), median_estimators]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: array\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        # Get the median prediction\n",
    "        return get_predict(estimator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_1 = GPABRegressor(estimator=gpab, n_estimators=gpab.pop_size, learning_rate=1.0, loss=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<gp.GP at 0x7f53af1aaf40>, 100, 1.0, 'linear')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_1.estimator, regr_1.n_estimators, regr_1.learning_rate, regr_1.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_X(gpab)\n",
    "y = get_y(gpab)\n",
    "data[\"weights_update\"] = 1.0 / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"weights_update\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "regr_1.fit(X, y, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69542                [teary, adler, tells, of, family]\n",
       "142293    [ethiopia, tigray, refugees, sudan, eritrea]\n",
       "188137           [vic, corruption, fighter, tells, of]\n",
       "123849                [fowler, fury, set, for, crisis]\n",
       "18781                 [love, pleads, guilty, to, drug]\n",
       "                              ...                     \n",
       "166473      [tendulkar, confident, ahead, of, special]\n",
       "265261               [russia, to, build, reactors, in]\n",
       "17271           [hotel, for, former, academy, cinemas]\n",
       "7032             [voss, out, fletcher, faces, nervous]\n",
       "52567       [abbott, backs, morrisons, asylum, seeker]\n",
       "Name: 0, Length: 2669, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpab.inputword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teary', 'adler', 'tells', 'of', 'family']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpab.inputword.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'protected_div(square(c), b)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(one_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function <lambda>(a, b, c, d, e)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_tree = gpab.pop[0]\n",
    "func = deap_gp.compile(one_tree, gpab.pset)\n",
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teary', 'adler', 'tells', 'of', 'family']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sentence = gpab.inputword.iloc[0]\n",
    "one_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([gpab.embeddings[char] for char in one_sentence])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.3991321e-01, -1.2687312e-02,  3.8116419e-01,  4.7464702e-02,\n",
       "       -6.3511804e-02, -3.0491473e-03,  6.4744306e-04,  1.7864481e+00,\n",
       "       -4.4006062e-01,  2.3497958e-03], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for words in gpab.inputword:\n",
    "    X = np.array([gpab.embeddings[char] for char in words])\n",
    "    y_pred = func(*X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669, 10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list = np.array([func(*np.array([gpab.embeddings[char] for char in words])) for words in gpab.inputword])\n",
    "y_pred_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([gpab.embeddings[char] for char in gpab.realword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669, 10)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_list = np.array(np.array([gpab.embeddings[char] for char in gpab.realword]))\n",
    "y_true_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpab.inputword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differences: (2669,)\n",
      "D: 11136956.0\n",
      "L_1: (2669,), L_2: (2669,), L_3: (2669,)\n",
      "L: (2669,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.04124756e-05, 1.67119513e-05, 1.08809853e-06, ...,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_weight(trees):\n",
    "    differences = np.zeros((len(trees.inputword)))\n",
    "    num_data = 0\n",
    "    for idx, tree in enumerate(trees.pop):\n",
    "        func = deap_gp.compile(tree, trees.pset)\n",
    "        # print(f\"tree: {tree}\")\n",
    "\n",
    "        y_preds_for_one_tree = np.array([func(*np.array([trees.embeddings[char] for char in words])) for words in trees.inputword])\n",
    "\n",
    "        y_trues_for_one_tree = np.array(np.array([trees.embeddings[char] for char in trees.realword]))\n",
    "\n",
    "        # Calculate the difference\n",
    "        differences_for_one_tree = np.linalg.norm((y_preds_for_one_tree - y_trues_for_one_tree), axis=1)\n",
    "        # print(f\"differences_for_one_tree: {differences_for_one_tree.shape}\")\n",
    "\n",
    "        differences[num_data] = differences_for_one_tree[0]\n",
    "\n",
    "        num_data += 1\n",
    "\n",
    "    print(f\"differences: {differences.shape}\")\n",
    "\n",
    "    # Find the supremum (maximum) of these differences\n",
    "    D = np.max(differences)\n",
    "    print(f\"D: {D}\")\n",
    "\n",
    "    L_1 = differences / D\n",
    "    L_2 = np.square(differences) / np.square(D)\n",
    "    L_3 = 1 - np.exp(-differences / D)\n",
    "    print(f\"L_1: {L_1.shape}, L_2: {L_2.shape}, L_3: {L_3.shape}\")\n",
    "    L = np.mean(np.stack((L_1, L_2, L_3), axis=0), axis=0)\n",
    "    print(f\"L: {L.shape}\")\n",
    "\n",
    "    return L\n",
    "\n",
    "result_L = update_weight(gpab)\n",
    "result_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = result_L / (1 - result_L)\n",
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"weights_update\"] = data[\"weights\"] * np.exp(beta * )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
