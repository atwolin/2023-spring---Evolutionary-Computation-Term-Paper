{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deap import gp as deap_gp\n",
    "import gp\n",
    "from data import get_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1126\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        algorithm,\n",
    "        embedding_type,\n",
    "        dimension,\n",
    "        population_size,\n",
    "        crossover_method,\n",
    "        cross_prob,\n",
    "        mut_prob,\n",
    "        num_generations,\n",
    "        num_evaluations,\n",
    "        debug,\n",
    "    ):\n",
    "        self.algorithm = algorithm\n",
    "        self.embedding_type = embedding_type\n",
    "        self.dimension = dimension\n",
    "        self.population_size = population_size\n",
    "        self.crossover_method = crossover_method\n",
    "        self.cross_prob = cross_prob\n",
    "        self.mut_prob = mut_prob\n",
    "        self.num_generations = num_generations\n",
    "        self.num_evaluations = num_evaluations\n",
    "        self.debug = debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = config(\"simple_gp\",\n",
    "        \"word2vec\",\n",
    "        10,\n",
    "        100,\n",
    "        \"cx_random\",\n",
    "        0.9,\n",
    "        0.1,\n",
    "        100,\n",
    "        1000,\n",
    "        False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "data, embeddings, embedding_model = get_embeddings(\n",
    "        Config.embedding_type, Config.dimension\n",
    "    )\n",
    "\n",
    "cx_method = gp.get_cx_num(Config.crossover_method)\n",
    "\n",
    "# Initialize instance weights\n",
    "data[\"weights\"] = 1.0 / len(data)\n",
    "data[\"weights_update\"] = 1.0 / len(data)\n",
    "\n",
    "iboost = 10  # Boosting interval\n",
    "ensemble = []  # Ensemble to store the best individuals\n",
    "loss = \"linear\"\n",
    "learning_rate = 1.0\n",
    "sample_weight = np.array(data[\"weights_update\"])\n",
    "num_ensemble = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpab = gp.GP(\n",
    "    Config.algorithm,\n",
    "    Config.embedding_type,\n",
    "    Config.dimension,\n",
    "    Config.population_size,\n",
    "    cx_method,\n",
    "    Config.cross_prob,\n",
    "    Config.mut_prob,\n",
    "    Config.num_generations,\n",
    "    Config.num_evaluations,\n",
    "    data,\n",
    "    embeddings,\n",
    ")\n",
    "gpab.initialize_pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(trees):\n",
    "        X_list = np.array([np.array([trees.embeddings[char] for char in words]) for words in trees.inputword])\n",
    "        return X_list\n",
    "\n",
    "def get_predict(trees, individual):\n",
    "        func = deap_gp.compile(individual, gpab.pset)\n",
    "        y_pred_list = np.array([func(*np.array([trees.embeddings[char] for char in words])) for words in trees.inputword])\n",
    "        return y_pred_list\n",
    "\n",
    "def get_y(trees):\n",
    "    y_true_list = np.array(np.array([trees.embeddings[char] for char in trees.realword]))\n",
    "    return y_true_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iboost = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample weight:  [0.00042234 0.00035713 0.00043287 ... 0.0004323  0.00047308 0.00042408]\n"
     ]
    }
   ],
   "source": [
    "gpab.n_gen = 0\n",
    "# while gpab.n_gen < gpab.max_gen:\n",
    "while gpab.n_gen < 2:\n",
    "    gpab.select()\n",
    "\n",
    "    epsilon = np.finfo(sample_weight.dtype).eps\n",
    "    zero_weight_mask = sample_weight == 0.0\n",
    "\n",
    "    # Boosting\n",
    "    if gpab.n_gen % iboost == 0:\n",
    "        # for iboost in range(len(num_ensemble)):\n",
    "\n",
    "        # Get the best individual\n",
    "        best_ind = max(gpab.pop, key=lambda x: x.fitness.values)\n",
    "        # Avoid extremely small weights\n",
    "        sample_weight = np.clip(sample_weight, a_min=epsilon, a_max=None)\n",
    "        sample_weight[zero_weight_mask] = 0.0\n",
    "\n",
    "        # Boosting step\n",
    "        X = get_X(gpab)\n",
    "        y = get_y(gpab)\n",
    "        sample_weight, estimator_weight, estimator_error = boosting(iboost, gpab, best_ind, X, y, sample_weight, learning_rate, loss)\n",
    "\n",
    "        # Early termination\n",
    "        if sample_weight is None:\n",
    "            break\n",
    "\n",
    "        # Stop if error is zero\n",
    "        if estimator_error == 0:\n",
    "            break\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "        if not np.isfinite(sample_weight_sum):\n",
    "            warnings.warn(\n",
    "                (\n",
    "                    \"Sample weights have reached infinite values,\"\n",
    "                    f\" at iteration {iboost}, causing overflow. \"\n",
    "                    \"Iterations stopped. Try lowering the learning rate.\"\n",
    "                ),\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            break\n",
    "\n",
    "        # Stop if the sum of sample weights has become non-positive\n",
    "        if sample_weight_sum <= 0:\n",
    "            break\n",
    "\n",
    "        if iboost < num_ensemble - 1:\n",
    "            # Normalize\n",
    "            sample_weight /= sample_weight_sum\n",
    "\n",
    "        print(\"Sample weight: \", sample_weight)\n",
    "\n",
    "        # Update the population dataset\n",
    "        select_new_data = np.random.uniform(0, 1, len(data))\n",
    "        data[\"cumulative_weights\"] = data[\"weights_update\"].cumsum()\n",
    "        # Find the indices of the closest rows in cumulative_weights for each value in select_new_data\n",
    "        indices = np.digitize(select_new_data, data[\"cumulative_weights\"])\n",
    "        # Create new dataset by selecting rows from original dataset based on indices\n",
    "        new_dataset = data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "        gpab.data = new_dataset\n",
    "        # Evaluate the entire population\n",
    "        fitnesses = map(gpab.toolbox.evaluate, gpab.pop)\n",
    "        for ind, fit in zip(gpab.pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "    print(\"Generation: \", gpab.n_gen)\n",
    "    gpab.n_gen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00037467, 0.00037467, 0.00037467, ..., 0.00037467, 0.00037467,\n",
       "       0.00037467])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting(iboost, gpab, best_ind, X, y, sample_weight, learning_rate, loss):\n",
    "    y_pred = get_predict(gpab, best_ind)\n",
    "\n",
    "    error_vect = np.linalg.norm(y - y_pred, axis=1)\n",
    "    sample_mask = sample_weight > 0\n",
    "    masked_sample_weight = sample_weight[sample_mask]\n",
    "    masked_error_vector = error_vect[sample_mask]\n",
    "\n",
    "    error_max = masked_error_vector.max()\n",
    "    if error_max != 0:\n",
    "        masked_error_vector /= error_max\n",
    "\n",
    "    if loss == \"square\":\n",
    "        masked_error_vector **= 2\n",
    "    elif loss == \"exponential\":\n",
    "        masked_error_vector = 1.0 - np.exp(-masked_error_vector)\n",
    "\n",
    "    # Culcalate the average loss\n",
    "    estimator_error = (masked_sample_weight * masked_error_vector).sum()\n",
    "    if estimator_error <= 0:\n",
    "        # Stop if fit is perfect\n",
    "        return sample_weight, 1.0, 0.0\n",
    "    elif estimator_error >= 0.5:\n",
    "        # Discard the estimator if worse than random guessing and it isn't the only one\n",
    "        if len(ensemble) > 0:\n",
    "            ensemble.pop(-1)\n",
    "        return None, None, None\n",
    "\n",
    "    beta = estimator_error / (1.0 - estimator_error)\n",
    "\n",
    "    # Boost weight using AdaBoost.R2 algorithm\n",
    "    estimator_weight = learning_rate * np.log(1.0 / beta)\n",
    "\n",
    "    if not iboost == num_ensemble - 1:\n",
    "        sample_weight[sample_mask] *= np.power(\n",
    "        beta, (1.0 - masked_error_vector) * learning_rate\n",
    "    )\n",
    "\n",
    "    return sample_weight, estimator_weight, estimator_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sample_weight = np.array(data[\"weights_update\"])\n",
    "sample_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_predict(gpab)\n",
    "y = get_y(gpab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 1\n",
    "error_vect = np.linalg.norm(y - y_pred, axis=1)\n",
    "error_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 2\n",
    "sample_mask = sample_weight > 0\n",
    "sample_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 3\n",
    "masked_sample_weight = sample_weight[sample_mask]\n",
    "masked_sample_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 4\n",
    "masked_error_vector = error_vect[sample_mask]\n",
    "masked_error_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 5\n",
    "error_max = masked_error_vector.max()\n",
    "error_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082835.4"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8427035e-05, 4.3844911e-06, 1.7800233e-04, ..., 1.4145022e-03,\n",
       "       6.5471912e-05, 9.6160511e-06], dtype=float32)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 6\n",
    "if error_max != 0:\n",
    "    masked_error_vector /= error_max\n",
    "masked_error_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line 7\n",
    "if loss == \"square\":\n",
    "    masked_error_vector **= 2\n",
    "elif loss == \"exponential\":\n",
    "    masked_error_vector = 1.0 - np.exp(-masked_error_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006279703240808366"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 8\n",
    "estimator_error = (masked_sample_weight * masked_error_vector).sum()\n",
    "estimator_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (2712928142.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[307], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    return sample_weight\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# line 9\n",
    "# Calculate the average loss\n",
    "if estimator_error <= 0:\n",
    "    # Stop if fit is perfect\n",
    "    return sample_weight\n",
    "elif estimator_error >= 0.5:\n",
    "    # Discard the estimator if worse than random guessing and it isn't the only one\n",
    "    if len(ensemble) > 0:\n",
    "        ensemble.pop(-1)\n",
    "    return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006283649186024124"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 10\n",
    "beta = estimator_error / (1.0 - estimator_error)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.372389479680931"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line 11\n",
    "estimator_weight = learning_rate * np.log(1.0 / beta)\n",
    "estimator_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line 12\n",
    "if not iboost == len(ensemble) - 1:\n",
    "    sample_weight[sample_mask] *= np.power(\n",
    "        beta, (1.0 - masked_error_vector) * learning_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line 13\n",
    "return sample_weight, estimator_weight, estimator_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPABRegressor():\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: deap.gp object\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "\n",
    "    n_estimators: int\n",
    "        The number of estimators to train, a.k.a. the number of population of GP trees.\n",
    "\n",
    "    learning_rate: float, default=1.0\n",
    "        The learning rate of the boosting algorithm.\n",
    "\n",
    "    loss: {'linear', 'square', 'exponential'}, optional\n",
    "        The loss function to use when updating the weights after each iteration.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimators_: estimator\n",
    "        The base estimator from which the ensemble is grown.\n",
    "\n",
    "    estimators_: list of regressors\n",
    "        The collection of fitted sub-estimators.\n",
    "\n",
    "    estimator_weights_: array-like of shape (n_estimators,)\n",
    "        Weights for each estimator in the boosted ensemble.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, n_estimators, learning_rate=1, loss=\"linear\"):\n",
    "        self.estimator = estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Build a boosted ensemble of estimators from the training set (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array\n",
    "            The training input samples (length = 5)\n",
    "\n",
    "        y: array\n",
    "            The target values (real numbers)\n",
    "\n",
    "        sample_weight: array\n",
    "            The sample weights. If None, the sample weights are initialized to 1 / n_samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # sample_weight /= sample_weight.sum()\n",
    "\n",
    "        # Clear any previous fit\n",
    "        # self.estimators_ = []\n",
    "        # self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        # self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "        epsilon = np.finfo(sample_weight).eps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        zero_weight_mask = sample_weight == 0\n",
    "        for iboost in range(self.n_estimators):\n",
    "\n",
    "            # Avoid extremely small sample weight\n",
    "            sample_weight = np.clip(sample_weight, a_min=epsilon, a_max=None)\n",
    "            sample_weight[zero_weight_mask] = 0.0\n",
    "\n",
    "            # Boosting step\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(iboost, X, y, sample_weight)\n",
    "\n",
    "            # Early stopping\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "            if not np.isfinite(sample_weight_sum):\n",
    "                warnings.warn(\n",
    "                    (\n",
    "                        \"Sample weights have reached infinite values,\"\n",
    "                        f\" at iteration {iboost}, causing overflow. \"\n",
    "                        \"Iterations stopped. Try lowering the learning rate.\"\n",
    "                    ),\n",
    "                    stacklevel=2,\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize the sample weights\n",
    "                sample_weight /= sample_weight_sum\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _boost(self, iboost, X, y, sample_weight):\n",
    "        \"\"\"\n",
    "        Implement a single boost iteration.\n",
    "\n",
    "        Perform a single boost according to the AdaBoost.R2 algorithm and return the updated sample weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        iboost: int\n",
    "            The current boosting iteration.\n",
    "\n",
    "        X: array\n",
    "            The training input samples (length = 5)\n",
    "\n",
    "        y: array\n",
    "            The target values (real numbers)\n",
    "\n",
    "        sample_weight: array\n",
    "            The current sample weights.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample_weight: array\n",
    "            The updated sample weights.\n",
    "\n",
    "        estimator_weight: float\n",
    "            The weight of the estimator.\n",
    "\n",
    "        estimator_error: float\n",
    "            The error of the estimator.\n",
    "        \"\"\"\n",
    "        estimator = self.estimator\n",
    "\n",
    "        # Weighted sampling of the training data with replacement\n",
    "        # boostrap_idx = np.random.choice(random_state, size=len(X), replace=True, p=sample_weight)\n",
    "\n",
    "\n",
    "        # Fit on the bootstrapped sample and obtain the predictions\n",
    "        # estimator.fit(X, y)\n",
    "        # estimator.select()\n",
    "        y_pred = self.predict(estimator)\n",
    "\n",
    "        error_vect = np.abs(y_pred - y)\n",
    "        sample_mask = sample_weight > 0\n",
    "        masked_sample_weight = sample_weight[sample_mask]\n",
    "        masked_error_vector = error_vect[sample_mask]\n",
    "\n",
    "        error_max = np.max(error_vect[sample_mask])\n",
    "        if error_max != 0:\n",
    "            masked_error_vector /= error_max\n",
    "\n",
    "        if self.loss == \"square\":\n",
    "            masked_error_vector **= 2\n",
    "        elif self.loss == \"exponential\":\n",
    "            masked_error_vector = 1.0 - np.exp(-masked_error_vector)\n",
    "\n",
    "        # Calculate the average loss\n",
    "        estimator_error = (masked_sample_weight * masked_error_vector).sum()\n",
    "\n",
    "        if estimator_error <= 0:\n",
    "            # Stop if fit is perfect\n",
    "            return sample_weight, 1.0, 0.0\n",
    "        elif estimator_error >= 0.5:\n",
    "            # Discard current estimator only if it isn't the only one\n",
    "            if len(self.estimators_) > 1:\n",
    "                self.estimators_.pop(-1)\n",
    "            return None, None, None\n",
    "\n",
    "        beta = estimator_error / (1.0 - estimator_error)\n",
    "\n",
    "        # Boost weight using AdaBoost.R2 algo\n",
    "        estimator_weight = self.learning_rate * np.log(1.0 / beta)\n",
    "\n",
    "        if not iboost == self.n_estimators - 1:\n",
    "            # Update the sample weights\n",
    "            sample_weight[sample_mask] *= np.power(beta, 1.0 - masked_error_vector * self.learning_rate)\n",
    "\n",
    "    # def _get_median_predict(self, X, limit):\n",
    "        # Evaluate predictions of all estimators (ensemble)\n",
    "        # predictions = np.array([estimator.predict(X) for estimator in self.estimators_[:limit]])\n",
    "        # predictions = np.array([func(*np.array([gpab.embeddings[char] for char in words])) for words in gpab.inputword])\n",
    "\n",
    "        # # Sort the predictions\n",
    "        # sorted_idx = np.argsort(predictions, axis=1)\n",
    "\n",
    "        # # Find index of median prediction for each sample\n",
    "        # weight_cdf = np.cumsum(self.estimator_weights_[:limit], axis=1)\n",
    "        # median_or_above = weight_cdf >= 0.5\n",
    "        # median_idx = np.argmax(median_or_above, axis=1)\n",
    "\n",
    "        # median_estimators = sorted_idx[np.arange(len(X), median_idx)]\n",
    "\n",
    "        # # Return the median prediction\n",
    "        # return predictions[np.arange(len(X)), median_estimators]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: array\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        # Get the median prediction\n",
    "        return get_predict(estimator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_1 = GPABRegressor(estimator=gpab, n_estimators=gpab.pop_size, learning_rate=1.0, loss=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<gp.GP at 0x7f53af1aaf40>, 100, 1.0, 'linear')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_1.estimator, regr_1.n_estimators, regr_1.learning_rate, regr_1.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_X(gpab)\n",
    "y = get_y(gpab)\n",
    "data[\"weights_update\"] = 1.0 / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"weights_update\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "regr_1.fit(X, y, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69542                [teary, adler, tells, of, family]\n",
       "142293    [ethiopia, tigray, refugees, sudan, eritrea]\n",
       "188137           [vic, corruption, fighter, tells, of]\n",
       "123849                [fowler, fury, set, for, crisis]\n",
       "18781                 [love, pleads, guilty, to, drug]\n",
       "                              ...                     \n",
       "166473      [tendulkar, confident, ahead, of, special]\n",
       "265261               [russia, to, build, reactors, in]\n",
       "17271           [hotel, for, former, academy, cinemas]\n",
       "7032             [voss, out, fletcher, faces, nervous]\n",
       "52567       [abbott, backs, morrisons, asylum, seeker]\n",
       "Name: 0, Length: 2669, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpab.inputword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teary', 'adler', 'tells', 'of', 'family']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpab.inputword.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'protected_div(square(c), b)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(one_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function <lambda>(a, b, c, d, e)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_tree = gpab.pop[0]\n",
    "func = deap_gp.compile(one_tree, gpab.pset)\n",
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teary', 'adler', 'tells', 'of', 'family']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sentence = gpab.inputword.iloc[0]\n",
    "one_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([gpab.embeddings[char] for char in one_sentence])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.3991321e-01, -1.2687312e-02,  3.8116419e-01,  4.7464702e-02,\n",
       "       -6.3511804e-02, -3.0491473e-03,  6.4744306e-04,  1.7864481e+00,\n",
       "       -4.4006062e-01,  2.3497958e-03], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for words in gpab.inputword:\n",
    "    X = np.array([gpab.embeddings[char] for char in words])\n",
    "    y_pred = func(*X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669, 10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list = np.array([func(*np.array([gpab.embeddings[char] for char in words])) for words in gpab.inputword])\n",
    "y_pred_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([gpab.embeddings[char] for char in gpab.realword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669, 10)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_list = np.array(np.array([gpab.embeddings[char] for char in gpab.realword]))\n",
    "y_true_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpab.inputword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differences: (2669,)\n",
      "D: 11136956.0\n",
      "L_1: (2669,), L_2: (2669,), L_3: (2669,)\n",
      "L: (2669,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.04124756e-05, 1.67119513e-05, 1.08809853e-06, ...,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_weight(trees):\n",
    "    differences = np.zeros((len(trees.inputword)))\n",
    "    num_data = 0\n",
    "    for idx, tree in enumerate(trees.pop):\n",
    "        func = deap_gp.compile(tree, trees.pset)\n",
    "        # print(f\"tree: {tree}\")\n",
    "\n",
    "        y_preds_for_one_tree = np.array([func(*np.array([trees.embeddings[char] for char in words])) for words in trees.inputword])\n",
    "\n",
    "        y_trues_for_one_tree = np.array(np.array([trees.embeddings[char] for char in trees.realword]))\n",
    "\n",
    "        # Calculate the difference\n",
    "        differences_for_one_tree = np.linalg.norm((y_preds_for_one_tree - y_trues_for_one_tree), axis=1)\n",
    "        # print(f\"differences_for_one_tree: {differences_for_one_tree.shape}\")\n",
    "\n",
    "        differences[num_data] = differences_for_one_tree[0]\n",
    "\n",
    "        num_data += 1\n",
    "\n",
    "    print(f\"differences: {differences.shape}\")\n",
    "\n",
    "    # Find the supremum (maximum) of these differences\n",
    "    D = np.max(differences)\n",
    "    print(f\"D: {D}\")\n",
    "\n",
    "    L_1 = differences / D\n",
    "    L_2 = np.square(differences) / np.square(D)\n",
    "    L_3 = 1 - np.exp(-differences / D)\n",
    "    print(f\"L_1: {L_1.shape}, L_2: {L_2.shape}, L_3: {L_3.shape}\")\n",
    "    L = np.mean(np.stack((L_1, L_2, L_3), axis=0), axis=0)\n",
    "    print(f\"L: {L.shape}\")\n",
    "\n",
    "    return L\n",
    "\n",
    "result_L = update_weight(gpab)\n",
    "result_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669,)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = result_L / (1 - result_L)\n",
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"weights_update\"] = data[\"weights\"] * np.exp(beta * )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
